Title,Subcategory,Link,Published,Authors,Summary
Advancing AI Trustworthiness Through Patient Simulation: Risk Assessment of Conversational Agents for Antidepressant Selection,"Judge Reliability And Calibration, Robustness And Sensitivity",[arXiv](http://arxiv.org/abs/2602.11391v1),2026-02-11,"Md Tanvir Rouf Shawon, Mohammad Sabik Irbaz, Hadeel R. A. Elyazori, et al.","Purpose: This paper introduces a patient simulator designed to enable scalable, automated evaluation of healthcare conversational agents through realistic and controllable patient interactions. Method: The simulator is grounded in the NIST AI Risk Management Framework and integrates three profile components reflecting different dimensions of patient variation, including medical, linguistic, and behavioral profiles. Results: The simulator effectively identified errors in an AI decision aid for antidepressant selection, revealing a degradation in performance across the health literacy spectrum with rank-one concept retrieval accuracy increasing from 47.9% to 81.6% as health literacy improved."
Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge,"Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.11340v1),2026-02-11,"Bo Pan, Xuan Kan, Kaitai Zhang, et al.","Purpose: The purpose of this work is to improve the alignment of large language model (LLM) evaluations with human judgments, particularly in multimodal settings where AI-generated images are evaluated. Method: The method proposed in this paper, called Bi-Level Prompt Optimization (BLPO), converts images into textual representations and jointly refines the judge prompt and the image-to-text (I2T) prompt to maintain fidelity under limited context budgets. Results: The results of experiments on four datasets and three LLM judges demonstrate the effectiveness of the BLPO framework in evaluating AI-generated images, offering a more efficient alternative to supervised fine-tuning on human-labeled data."
CryptoAnalystBench: Failures in Multi-Tool Long-Form LLM Analysis,"Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.11304v1),2026-02-11,"Anushri Eswaran, Oleg Golev, Darshan Tank, et al.","Purpose: The paper investigates the failure modes of large language models (LLMs) when analyzing complex, high-token inputs from multiple tools in a representative domain, specifically crypto and DeFi queries. Method: The authors introduce CryptoAnalystBench, a benchmark of 198 production crypto and DeFi queries, an agentic harness to generate responses across multiple LLMs, and an evaluation pipeline with citation verification and a judge rubric to assess the quality of the responses. Results: The study finds that even state-of-the-art systems exhibit persistent failures that can compromise high-stakes decisions, and proposes a refined judge rubric and error taxonomy to better capture these errors, enabling scalable feedback for developers and researchers."
FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight,"Metrics And Scoring Methods, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.11136v2),2026-02-11,"Jiayi Zhou, Yang Sheng, Hantao Lou, et al.","Purpose: The paper aims to address the dilemma of ensuring behavioral safety in large language model (LLM)-based agents operating in high-stakes domains by proposing a neuro-symbolic framework for agentic oversight. Method: The proposed framework, FormalJudge, employs a bidirectional Formal-of-Thought architecture that uses LLMs as specification compilers to translate natural language requirements into formal specifications and verify compliance using Dafny and Z3 solvers. Results: The experiments demonstrate that FormalJudge achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization, and provides near-linear safety improvement through iterative refinement across three benchmarks and seven agent models."
In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution,Benchmark And Dataset Creation,[arXiv](http://arxiv.org/abs/2602.11079v2),2026-02-11,"Frank Xiao, Santiago Aranguri","Purpose: The paper proposes a method to mitigate undesirable emergent behaviors in production language models by tracing behavioral changes to responsible training datapoints using activation-based data attribution. Method: By computing activation-difference vectors and ranking by cosine similarity, the authors identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Results: The proposed method is shown to be effective in reducing harmful behaviors, such as distractor-triggered compliance, by up to 78% while outperforming baseline methods at a lower computational cost."
S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage,"Judge Prompting Protocols, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.10606v2),2026-02-11,"Jie Jiang, Hongbo Tang, Wenjie Wu, et al.","Purpose: The purpose of this paper is to present S-GRec, a semantic-aware framework that addresses the limitations of generative recommendation models by incorporating Large Language Models (LLMs) to provide rich semantic priors for training. Method: Methodologically, S-GRec decouples an online lightweight generator from an offline LLM-based semantic judge and introduces a two-stage Personalized Semantic Judge (PSJ) to produce interpretable aspect evidence and learn user-conditional aggregation from pairwise feedback. Results: Results from extensive experiments on public benchmarks and a large-scale production system demonstrate the effectiveness and scalability of S-GRec, including statistically significant gains in CTR and a 1.19% lift in GMV in online A/B tests."
RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty,"Judge Reliability And Calibration, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.12424v1),2026-02-12,"Ziqian Zhang, Xingjian Hu, Yue Huang, et al.","Purpose: The paper proposes a novel framework called RankLLM to quantify both question difficulty and model competency in evaluating large language models (LLMs), addressing the limitation of existing benchmarks that fail to differentiate question difficulty. Method: The core mechanism of RankLLM facilitates bidirectional score propagation between models and questions, where a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Results: RankLLM achieves 90% agreement with human judgments, outperforms strong baselines, and exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation."
LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss,Metrics And Scoring Methods,[arXiv](http://arxiv.org/abs/2602.12005v2),2026-02-12,"Szilvia Ujváry, Louis Béthune, Pierre Ablin, et al.","Purpose: The paper investigates the fundamental question of what small language models can and should learn during pretraining, versus what they should delegate to an outside source to prevent factual errors. Method: The researchers propose a novel pretraining method called LaCy, which uses a spaCy grammar parser to augment the loss signal and decide which tokens the model should learn to delegate. Results: The experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help, resulting in higher FactScores and outperforming other methods while being simpler and cheaper."
LLM-based Triplet Extraction from Financial Reports,Metrics And Scoring Methods,[arXiv](http://arxiv.org/abs/2602.11886v1),2026-02-12,"Dante Wesslund, Ville Stenström, Pontus Linde, et al.","Purpose: The paper aims to present a semi-automated pipeline for extracting Subject-Predicate-Object triplets from corporate financial reports using Large Language Models (LLMs) and ontology-driven proxy metrics for evaluation. Method: The approach uses a combination of static and automated ontology induction methods, as well as a hybrid verification strategy that combines regex matching with LLM-based judgment to extract accurate triplets from financial reports. Results: The automatically induced ontology achieves 100% schema conformance and reduces subject hallucination rates from 65.2% to 1.6%, while also revealing a systematic asymmetry between subject and object hallucinations in financial prose."
Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing,"Judge Reliability And Calibration, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.11786v1),2026-02-12,Keita Broadwater,"Purpose: The paper aims to evaluate the safety of large language models (LLMs) under repeated inference, which is a critical aspect in high-stakes settings where response consistency and safety are essential. Method: The researchers introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework that repeatedly samples identical prompts under controlled conditions to surface latent failure modes and estimate per-inference failure probabilities. Results: The application of APST to multiple instruction-tuned LLMs reveals substantial differences in empirical failure rates under repeated sampling, particularly as temperature increases, demonstrating the need for a practical framework to evaluate LLM safety and reliability under repeated inference."
Asynchronous Verified Semantic Caching for Tiered LLM Architectures,"Robustness And Sensitivity, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.13165v1),2026-02-13,"Asmit Kumar Singh, Haozhe Wang, Laxmi Naga Santosh Attaluri, et al.","Purpose: The paper aims to reduce inference cost and latency in large language models by introducing an asynchronous verified semantic caching policy for tiered architectures. Method: The proposed approach, called Krites, uses a static cache and dynamically invokes a language model judge to verify whether static responses are acceptable for new prompts that fall just below the static threshold. Results: Krites increases the fraction of requests served with curated static answers by up to 3.9 times for conversational traffic and search-style queries relative to tuned baselines, without changing critical path latency."
SCOPE: Selective Conformal Optimized Pairwise LLM Judging,"Judge Reliability And Calibration, Judge Prompting Protocols",[arXiv](http://arxiv.org/abs/2602.13110v2),2026-02-13,"Sher Badshah, Ali Emami, Hassan Sajjad","Purpose: The paper proposes a framework called SCOPE for selective pairwise judging using large language models (LLMs) as judges to replace human preference labels in pairwise evaluation with finite-sample statistical guarantees. Method: To achieve this, the authors introduce Bidirectional Preference Entropy (BPE), which provides a bias-neutral uncertainty signal by querying the judge under both response positions and aggregating the implied preference probabilities. Results: The proposed framework, SCOPE, consistently meets the target risk level while retaining good coverage across judge scales, demonstrating its effectiveness in reliable and high-coverage LLM-based evaluation with significant improvements over naïve baselines."
iRULER: Intelligible Rubric-Based User-Defined LLM Evaluation for Revision,"Judge Prompting Protocols, Domain-Specific Judging",[arXiv](http://arxiv.org/abs/2602.12779v1),2026-02-13,"Jingwen Bai, Wei Soon Cheong, Philippe Muller, et al.","Purpose: The purpose of the paper is to propose a novel approach called iRULER, which aims to improve the evaluation and revision process of Large Language Models (LLMs) by providing intelligible and user-defined feedback. Method: Method: The method used in this study involves designing iRULER based on identified guidelines that scaffold the review process with specific criteria, justification for score selection, and actionable revisions, and then testing it through controlled experiments on writing revision and rubric creation. Results: Results: The results of the study show that iRULER significantly improved validated LLM-judged review scores, was perceived as most helpful and aligned, and satisfied the design guidelines for user-defined feedback, contributing to the development of interactive rubric tools for intelligible LLM-based review and revision."
CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation,"Metrics And Scoring Methods, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.12639v1),2026-02-13,"Yiran Rex Ma, Yuxiao Ye, Huiyuan Xie","Purpose: The purpose of this paper is to introduce CLASE, a hybrid method for evaluating the stylistic quality of Chinese legal text generated by large language models, which aims to address the limitations of existing evaluation methods. Method: The method used in CLASE combines linguistic feature-based scores and experience-guided LLM-as-a-judge scores, learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts, to capture both surface-level features and implicit stylistic norms. Results: The results of the experiments show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods, providing interpretable score breakdowns and suggestions for improvements in legal text generation."
T2MBench: A Benchmark for Out-of-Distribution Text-to-Motion Generation,"Benchmark And Dataset Creation, Judge Prompting Protocols",[arXiv](http://arxiv.org/abs/2602.13751v1),2026-02-14,"Bin Yang, Rong Ou, Weisheng Xu, et al.","Purpose: The paper proposes a benchmark specifically designed for out-of-distribution text-to-motion evaluation to address the limitation of existing evaluations that focus on in-distribution textual inputs. Method: The benchmark includes a comprehensive analysis of 14 representative baseline models and two datasets derived from evaluation results, as well as a unified evaluation framework that integrates multiple evaluation methods. Results: The experimental results reveal that while different baseline models demonstrate strengths in certain areas, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation, highlighting the limitations of existing methods in out-of-distribution scenarios."
Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges,"Judge Reliability And Calibration, Judge Prompting Protocols",[arXiv](http://arxiv.org/abs/2602.13576v1),2026-02-14,"Ruomeng Ding, Yifei Pang, He Sun, et al.","Purpose: The paper aims to identify and explore a previously under-recognized vulnerability in evaluation and alignment pipelines for large language models, which relies on natural-language rubrics and validated benchmarks. Method: The researchers analyze the behavior of LLM-based judges guided by these rubrics and investigate how even minor edits to the rubrics can produce systematic shifts in a judge's preferences on target domains. Results: The study reveals that rubric-induced preference drift can lead to significant reductions in target-domain accuracy, propagating bias through alignment pipelines and resulting in persistent drift in model behavior, with accuracy decreases of up to 9.5% and 27.9% in certain cases."
Small Reward Models via Backward Inference,Judge Prompting Protocols,[arXiv](http://arxiv.org/abs/2602.13551v1),2026-02-14,"Yike Wang, Faeze Brahman, Shangbin Feng, et al.","Purpose: The purpose of this work is to propose a new approach to reward modeling called FLIP, which reformulates reward modeling through backward inference to improve flexibility and accessibility in non-verifiable domains. Method: Methodologically, the authors evaluate FLIP across four domains using 13 small language models, comparing its performance to LLM-as-a-Judge baselines and assessing its effectiveness in various scenarios. Results: Results show that FLIP outperforms baseline methods by an average of 79.6%, substantially improves downstream performance, and is particularly effective for longer outputs and robust to common forms of reward hacking."
Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity,Benchmark And Dataset Creation,[arXiv](http://arxiv.org/abs/2602.14130v1),2026-02-15,"Kazuo Yano, Jonghyeok Lee, Tae Ishitomi, et al.","Purpose: This paper proposes Algebraic Quantum Intelligence (AQI) as a new framework to overcome the limitations of large language models in generating genuinely creative outputs by enabling systematic expansion of semantic space. Method: The AQI framework is formulated as a noncommutative algebraic structure inspired by quantum theory, and its implementation involves extending a transformer-based language model with specialized operators to represent semantic states and their evolution. Results: The evaluation of the AQI system on creative reasoning benchmarks shows that it consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance, demonstrating its potential as a practical foundation for machine creativity."
Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric,"Judge Prompting Protocols, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.14069v1),2026-02-15,"Ruipeng Jia, Yunyi Yang, Yuxin Wu, et al.","Purpose: The paper aims to address the limitations of scalar reward models in reinforcement learning by introducing an open rubric system that scales reinforcement learning with pairwise adaptive rubric. Method: The Open Rubric System (OpenRS) is presented as a plug-and-play framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components. Results: The OpenRS system instantiates adaptive rubrics on the fly, performs criterion-wise pairwise comparisons, and aggregates criterion-level preferences externally, ultimately improving discriminability in open-ended settings and avoiding pointwise weighted scalarization."
From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection,"Domain-Specific Judging, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.14012v1),2026-02-15,"Youpeng Li, Fuxun Yu, Xinda Wang","Purpose: The paper aims to investigate the post-training pipeline for Large Language Model (LLM)-based vulnerability detection, exploring the impact of various techniques on model training and assessment. Method: The study employs a comprehensive approach, spanning from cold-start Supervised Fine-Tuning (SFT) to off-policy preference optimization and on-policy Reinforcement Learning (RL), to analyze the effects of data curation, stage interactions, reward mechanisms, and evaluation protocols. Results: The investigation yields several key findings, including the effectiveness of SFT based on rejection sampling, the importance of fine-grained reward signals, and the superior performance of models trained under on-policy RL, as well as the need for robust evaluation protocols such as LLM-as-a-Judge."
Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation,"Benchmark And Dataset Creation, Domain-Specific Judging",[arXiv](http://arxiv.org/abs/2602.14564v1),2026-02-16,"Shefayat E Shams Adib, Ahmed Alfey Sani, Ekramul Alam Esham, et al.","Purpose: The paper aims to compare the performance of five Large Language Models (LLMs) for medical question answering (QA) using a zero-shot evaluation methodology on the iCliniq dataset. Method: The comparison is done by deploying the LLMs and evaluating their performance using BLEU and ROUGE metrics without specialized fine-tuning, considering models such as Llama-3-8B-Instruct and GPT-5-mini. Results: The results show that larger models like Llama 3.3 70B Instruct outperform smaller models, with Llama-4-Maverick-17B exhibiting competitive results, highlighting the trade-offs between model size and performance in medical QA applications."
Synthetic Reader Panels: Tournament-Based Ideation with LLM Personas for Autonomous Publishing,"Judge Reliability And Calibration, Judge Prompting Protocols",[arXiv](http://arxiv.org/abs/2602.14433v1),2026-02-16,Fred Zimmerman,"Purpose: The purpose of this paper is to present a system for autonomous book ideation that utilizes synthetic reader panels composed of diverse LLM-instantiated reader personas to evaluate book concepts through structured tournament competitions. Method: The method involves creating panels that reflect target demographics with diversity constraints, and using these panels to compete book concepts in various tournament formats, with evaluations judged against weighted criteria and filtered through automated anti-slop checks. Results: The results show that synthetic reader panels can produce actionable demographic segmentation, identify structural content issues, and effectively filter out low-quality book concepts while enriching high-quality ones, as demonstrated through three case studies within a multi-imprint publishing operation."
"InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem","Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.14367v1),2026-02-16,"Shuofei Qiao, Yunxiang Wei, Xuehai Wang, et al.","Purpose: The paper introduces InnoEval, a deep innovation evaluation framework designed to address the limitations of existing idea evaluation methods by regarding idea evaluation as a knowledge-grounded, multi-perspective reasoning problem. Method: The approach involves applying a heterogeneous deep knowledge search engine to retrieve dynamic evidence from diverse online sources and utilizing an innovation review board with reviewers from distinct academic backgrounds for multi-dimensional evaluation. Results: Experiments demonstrate that InnoEval consistently outperforms baselines in various evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts, as evidenced by its performance on comprehensive datasets derived from authoritative peer-reviewed submissions."
Key Considerations for Domain Expert Involvement in LLM Design and Evaluation: An Ethnographic Study,"Judge Prompting Protocols, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.14357v1),2026-02-16,"Annalisa Szymanski, Oghenemaro Anuyah, Toby Jia-Jun Li, et al.","Purpose: The purpose of this paper is to examine the challenges and trade-offs in Large Language Model (LLM) development through an ethnographic study of a team building a pedagogical chatbot. Method: The method used in this research involves a 12-week ethnographic study where the researcher observed design and evaluation activities and conducted interviews with both developers and domain experts to identify key practices in LLM development. Results: Results of the study revealed four key practices that demonstrate the central role of domain expertise in shaping the system, including creating workarounds for data collection and adopting hybrid expert-developer-LLM evaluation strategies, which inform design opportunities for future LLM development workflows."
Multi-Objective Alignment of Language Models for Personalized Psychotherapy,"Judge Reliability And Calibration, Judge Prompting Protocols",[arXiv](http://arxiv.org/abs/2602.16053v1),2026-02-17,"Mehrab Beikzadeh, Yasaman Asadollah Salmanpour, Ashima Suvarna, et al.","Purpose: The purpose of this study is to develop a multi-objective alignment framework for language models in personalized psychotherapy to balance patient preferences with clinical safety. Method: The method used involves surveying individuals with lived mental health experience, collecting preference rankings, and training reward models for six therapeutic criteria using direct preference optimization. Results: The results show that the proposed multi-objective approach, MODPO, achieves a superior balance between empathy and safety, outperforming single-objective optimization and other methods, and is consistently preferred by blinded clinician evaluation."
*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation,"Judge Reliability And Calibration, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.15778v1),2026-02-17,"Quentin Lemesle, Léane Jourdan, Daisy Munson, et al.","Purpose: The purpose of this paper is to address the limitations of existing LLM-as-a-judge methods for evaluating automatically generated text, which are computationally expensive and require post-processing. Method: Method: The researchers build upon ParaPLUIE, a perplexity-based LLM-judge metric, and introduce *-PLUIE, task-specific prompting variants that estimate confidence over ""Yes/No"" answers without generating text. Results: Results: The experiments demonstrate that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost, making it a more effective and efficient evaluation method."
ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models,"Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.15758v1),2026-02-17,"Manav Nitin Kapadnis, Lawanya Baghel, Atharva Naik, et al.","Purpose: The paper introduces ChartEditBench, a benchmark for evaluating the ability of Multimodal Large Language Models (MLLMs) to support real-world exploratory data analysis through multi-turn chart editing interactions. Method: The benchmark comprises 5,000 difficulty-controlled modification chains and a rigorously human-verified subset, and uses a robust evaluation framework that combines execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Results: Experiments with state-of-the-art MLLMs using ChartEditBench reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, highlighting the need for improved grounded multimodal programming capabilities."
Quantifying construct validity in large language model evaluations,"Benchmark And Dataset Creation, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.15532v1),2026-02-17,Ryan Othniel Kearns,"Purpose: The paper aims to address the issue of construct validity in large language model (LLM) evaluations by developing a method to separate benchmark results from actual capabilities. Method: The study proposes a structured capabilities model that combines the strengths of latent factor models and scaling laws to extract interpretable and generalisable capabilities from LLM benchmark results. Results: The structured capabilities model outperforms existing approaches, demonstrating better explanatory and predictive power for quantifying construct validity in LLM evaluations through improved parsimonious fit indices and out-of-distribution benchmark prediction."
SecCodeBench-V2 Technical Report,"Benchmark And Dataset Creation, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.15485v2),2026-02-17,"Longfei Chen, Ji Zhao, Lanxiao Cui, et al.","Purpose: The paper introduces SecCodeBench-V2, a publicly released benchmark for evaluating the capabilities of Large Language Model copilots in generating secure code across various programming languages and security issues. Method: The benchmark comprises 98 generation and fix scenarios with function-level task formulation, providing executable proof-of-concept test cases for functional validation and security verification, and adopts a unified evaluation pipeline assessing models via dynamic execution. Results: SecCodeBench-V2 enables holistic and comparable evaluation of AI coding assistants' security posture through a Pass@K-based scoring protocol, providing a rigorous and reproducible foundation with results and artifacts publicly available."
LLM-as-Judge on a Budget,"Benchmark And Dataset Creation, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.15481v1),2026-02-17,"Aadirupa Saha, Aniket Wagde, Branislav Kveton","Purpose: The paper aims to address the challenge of optimally allocating queries across prompt-response pairs to minimize estimation error when using large language models as judges, given a fixed computational budget. Method: The approach presented in the paper leverages multi-armed bandit theory and concentration inequalities to dynamically allocate queries based on estimated score variances, concentrating resources where uncertainty is highest. Results: The proposed method achieves a significant reduction in worst-case estimation error compared to uniform allocation, while maintaining identical budgets, as demonstrated through experiments on Summarize-From-Feedback and HelpSteer2 datasets."
SourceBench: Can AI Answers Reference Quality Web Sources?,"Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.16942v1),2026-02-18,"Hexi Jin, Stephen Liu, Yuheng Li, et al.","Purpose: The paper introduces SourceBench, a benchmark for measuring the quality of cited web sources used by large language models (LLMs) to answer queries, with the goal of evaluating evidence quality rather than just answer correctness. Method: The SourceBench framework uses eight metrics to assess content quality and page-level signals, and includes a human-labeled dataset with a calibrated LLM-based evaluator to evaluate the cited sources. Results: The evaluation of eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench reveals four key new insights that can guide future research in GenAI and web search."
Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark,"Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.16811v1),2026-02-18,"Charalampos Mastrokostas, Nikolaos Giarelis, Nikos Karacapilidis","Purpose: The study aims to address the research gap in Greek Question Answering by evaluating the effectiveness of monolingual and multilingual Large Language Models on language-specific tasks. Method: The evaluation is conducted using DemosQA, a novel dataset constructed from social media user questions and community-reviewed answers, as well as a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages. Results: The study presents an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies, with the code and data released to facilitate reproducibility."
References Improve LLM Alignment in Non-Verifiable Domains,Judge Reliability And Calibration,[arXiv](http://arxiv.org/abs/2602.16802v1),2026-02-18,"Kejian Shi, Yixin Liu, Peifeng Wang, et al.","Purpose: The paper investigates whether reference-guided LLM-evaluators can bridge the gap in non-verifiable domains by serving as soft ""verifiers"" for LLM alignment, where Reinforcement Learning with Verifiable Rewards cannot be directly applied. Method: The researchers design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs and conduct comprehensive experiments to test the effectiveness of a reference-guided approach. Results: The study shows that a reference-guided approach substantially improves the accuracy of LLM-judges, yielding clear gains over direct SFT on reference outputs and self-improvement with reference-free judges, and achieving performance comparable to training with strong finetuned reward models."
Who can we trust? LLM-as-a-jury for Comparative Assessment,"Judge Reliability And Calibration, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.16610v1),2026-02-18,"Mengjie Qian, Guangzhi Sun, Mark J. F. Gales, et al.","Purpose: The purpose of this paper is to assess the reliability of large language models (LLMs) as automatic evaluators for natural language generation assessment, particularly in pairwise comparative judgements. Method: Methodologically, the authors propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Results: Results of experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods and effectively models judge reliability through an unsupervised calibration mechanism."
CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes,"Benchmark And Dataset Creation, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.16607v1),2026-02-18,"Miguel Marques, Ana Luísa Fernandes, Ana Filipa Pacheco, et al.","Purpose: The purpose of this paper is to address the challenge of navigating lengthy and dense municipal meeting minutes by presenting a new corpus called CitiLink-Summ for automatic summarization of discussion subjects in European Portuguese. Method: Methodologically, the authors leverage the CitiLink-Summ dataset, comprising 100 documents and 2,322 manually crafted summaries, to establish baseline results using state-of-the-art generative models and large language models evaluated with various metrics. Results: Results from this study provide the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts through the establishment of baseline results for automatic summarization."
Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents,"Judge Reliability And Calibration, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.16246v1),2026-02-18,"Yun-Shiuan Chuang, Chaitanya Kulkarni, Alec Chiu, et al.","Purpose: The paper aims to propose a novel evaluation framework for multi-turn tool-calling large language model agents that can reliably compare models and yield on-policy training data in a scalable manner. Method: The proposed approach, called Proxy State-Based Evaluation, utilizes an LLM-driven simulation framework that preserves final state-based evaluation without requiring a deterministic database, instead relying on scenario specifications and LLM state trackers to infer proxy states. Results: The empirical results show that the benchmark produces stable and model-differentiating rankings, provides effective supervision for unseen scenarios, and achieves high human-LLM judge agreement, indicating a reliable and scalable alternative to traditional deterministic agentic benchmarks."
LiveClin: A Live Clinical Benchmark without Leakage,"Judge Reliability And Calibration, Domain-Specific Judging",[arXiv](http://arxiv.org/abs/2602.16747v1),2026-02-18,"Xidong Wang, Shuqi Guo, Yue Shen, et al.","Purpose: The paper introduces LiveClin, a live clinical benchmark designed to address the challenges of data contamination and knowledge obsolescence in evaluating medical large language models (LLMs). Method: Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin uses a verified AI-human workflow involving physicians to transform authentic patient cases into complex evaluation scenarios. Results: The evaluation of 26 models on LiveClin reveals the difficulty of real-world clinical scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%, and human experts, particularly Chief Physicians, surpassing most models in accuracy."
Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models,"Benchmark And Dataset Creation, Robustness And Sensitivity",[arXiv](http://arxiv.org/abs/2602.17433v1),2026-02-19,"Francesco Ortu, Joeun Yook, Punya Syon Pandey, et al.","Purpose: The paper aims to detect historical revisionism in large language models, which are increasingly used as sources of historical information, by introducing a curated dataset and evaluating the models' performance in various prompt scenarios. Method: The researchers use an LLM-as-a-judge protocol to compare model outputs to factual and revisionist reference narratives across different model architectures and prompt conditions, including neutral and robustness prompts. Results: The evaluation shows that while models are generally closer to factual references under neutral prompts, they exhibit sharply higher revisionism scores when prompted with robustness requests, indicating limited resistance to revisionist framing."
"Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation","Robustness And Sensitivity, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.17316v1),2026-02-19,"Bogdan Kostić, Conor Fallon, Julian Risch, et al.","Purpose: The paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary Large Language Models (LLMs) across three benchmarks. Method: The researchers employ two linguistically principled pipelines to generate meaning-preserving variations, one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results: The results show that lexical perturbations induce substantial performance degradation, while syntactic perturbations have heterogeneous effects, and both types destabilize model leaderboards, revealing that LLMs rely more on surface-level lexical patterns than abstract linguistic competence."
On the Reliability of User-Centric Evaluation of Conversational Recommender Systems,Judge Reliability And Calibration,[arXiv](http://arxiv.org/abs/2602.17264v1),2026-02-19,"Michael Müller, Amir Reza Mohammadi, Andreas Peintner, et al.","Purpose: The paper aims to investigate the reliability of user-centric evaluation of Conversational Recommender Systems (CRS) through a large-scale empirical study on static dialogue transcripts. Method: The study collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework and analyzed them using random-effects reliability models and correlation analysis. Results: The results show that while utilitarian dimensions achieve moderate reliability, socially grounded constructs are less reliable, and many dimensions collapse into a single global quality signal due to a strong halo effect in third-party judgments."
Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study,"Judge Reliability And Calibration, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.17262v1),2026-02-19,"Kensuke Okada, Yui Furukawa, Kyosuke Bunji","Purpose: The study aims to quantify and mitigate socially desirable responding in large language models (LLMs) when using human self-report questionnaires for evaluation, as these instruments assume honest responding but can be biased by socially preferred answers. Method: To address this issue, the researchers propose a psychometric framework that involves administering the same inventory under different instructions and computing socially desirable responding as a standardized effect size from item response theory-estimated latent scores. Results: The study finds that using a desirability-matched graded forced-choice inventory substantially attenuates socially desirable responding in LLMs while preserving the recovery of intended persona profiles, highlighting the need for socially desirable responding-aware reporting practices in questionnaire-based benchmarking and auditing of LLMs."
When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment,"Judge Reliability And Calibration, Judge Prompting Protocols",[arXiv](http://arxiv.org/abs/2602.17170v1),2026-02-19,"Chuting Yu, Hang Li, Joel Mackenzie, et al.","Purpose: The purpose of this study is to explore the reliability and stability of large language models (LLMs) as proxies for human judges in relevance assessment, examining their tendency to overrate scores. Method: Methodologically, the researchers conducted a systematic study of LLM-based relevance judgments across various model backbones, evaluation paradigms, and passage modification strategies to investigate overrating behavior. Results: Results show that LLMs consistently assign inflated relevance scores to passages that do not genuinely satisfy the underlying information need, revealing a system-wide bias and highlighting the need for careful diagnostic evaluation frameworks."
Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models,"Benchmark And Dataset Creation, Robustness And Sensitivity",[arXiv](http://arxiv.org/abs/2602.17433v1),2026-02-19,"Francesco Ortu, Joeun Yook, Punya Syon Pandey, et al.","Purpose: The purpose of this study is to introduce a method for detecting historical revisionism in large language models, which are increasingly used as sources of historical information. Method: The method involves using a curated dataset called HistoricalMisinfo, which contains 500 contested events from 45 countries, each paired with factual and revisionist reference narratives, and evaluating language models under neutral and robustness prompts. Results: The results show that while language models generally produce outputs closer to factual references under neutral prompts, they exhibit sharply higher revisionism scores when explicitly requested to provide the revisionist narrative, indicating limited resistance to revisionist framing."
"Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation","Robustness And Sensitivity, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.17316v1),2026-02-19,"Bogdan Kostić, Conor Fallon, Julian Risch, et al.","Purpose: The paper examines how controlled lexical and syntactic perturbations affect the performance and relative ranking of Large Language Models (LLMs) across various benchmarks. Method: The study employs two linguistically principled pipelines to generate meaning-preserving variations, including synonym substitution for lexical changes and dependency parsing for syntactic transformations. Results: The results show that lexical perturbations induce substantial performance degradation, while syntactic perturbations have heterogeneous effects, and both types destabilize model leaderboards, revealing a need for robustness testing in LLM evaluation."
On the Reliability of User-Centric Evaluation of Conversational Recommender Systems,Judge Reliability And Calibration,[arXiv](http://arxiv.org/abs/2602.17264v1),2026-02-19,"Michael Müller, Amir Reza Mohammadi, Andreas Peintner, et al.","Purpose: The paper aims to investigate the reliability of user-centric evaluation of Conversational Recommender Systems (CRS) through a large-scale empirical study on static dialogue transcripts. Method: The study collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework and analyzed them using random-effects reliability models and correlation analysis. Results: The results show that while utilitarian dimensions achieve moderate reliability, socially grounded constructs are less reliable, and many dimensions collapse into a single global quality signal due to a strong halo effect in third-party judgments."
Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study,"Judge Reliability And Calibration, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.17262v1),2026-02-19,"Kensuke Okada, Yui Furukawa, Kyosuke Bunji","Purpose: The study aims to quantify and mitigate socially desirable responding in large language models (LLMs) when using human self-report questionnaires for evaluation, as these instruments presume honest responding but can be biased by socially preferred answers. Method: The researchers propose a psychometric framework that administers the same inventory under HONEST versus FAKE-GOOD instructions to quantify socially desirable responding and construct a graded forced-choice inventory to mitigate it. Results: The study finds that desirability-matched graded forced-choice questionnaires substantially attenuate socially desirable responding in LLMs while preserving the recovery of intended persona profiles, highlighting a model-dependent trade-off between socially desirable responding and recovery."
When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment,"Judge Reliability And Calibration, Judge Prompting Protocols",[arXiv](http://arxiv.org/abs/2602.17170v1),2026-02-19,"Chuting Yu, Hang Li, Joel Mackenzie, et al.","Purpose: The paper aims to explore the reliability and stability of large language models (LLMs) as proxies for human judges in relevance assessment, by investigating their overrating behavior across different model backbones and evaluation paradigms. Method: The study conducts a systematic analysis of LLM-based relevance judgments using various passage modification strategies, pointwise and pairwise evaluation paradigms, and controlled experiments to examine the sensitivity of models to passage length and lexical cues. Results: The results show that LLMs consistently assign inflated relevance scores to passages that do not genuinely satisfy the underlying information need, revealing a system-wide bias and highlighting the need for careful diagnostic evaluation frameworks when using LLMs for relevance assessments."
The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI,"Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.17127v1),2026-02-19,Dusan Bosnjakovic,"Purpose: The paper aims to introduce a novel auditing framework for detecting durable, provider-level behavioral signatures in Large Language Models (LLMs) to ensure safety and governance in multi-agent systems. Method: The research utilizes psychometric measurement theory, specifically latent trait estimation under ordinal uncertainty, and employs techniques such as forced-choice ordinal vignettes and Mixed Linear Models (MixedLM) analysis to quantify latent tendencies in LLMs. Results: The study finds that a persistent ""lab signal"" accounts for significant behavioral clustering in leading models, indicating that latent biases are compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures."
SourceBench: Can AI Answers Reference Quality Web Sources?,"Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.16942v1),2026-02-18,"Hexi Jin, Stephen Liu, Yuheng Li, et al.","Purpose: The paper introduces SourceBench, a benchmark for measuring the quality of cited web sources used by large language models to answer queries, with the goal of evaluating evidence quality rather than just answer correctness. Method: The SourceBench framework uses eight metrics to assess content quality and page-level signals, and includes a human-labeled dataset with a calibrated LLM-based evaluator to evaluate the quality of cited web sources across various queries and intents. Results: The evaluation of eight large language models, Google Search, and three AI search tools using SourceBench reveals key insights that can guide future research in GenAI and web search, including the quality of cited web sources and the effectiveness of different models."
Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark,"Benchmark And Dataset Creation, Judge Reliability And Calibration",[arXiv](http://arxiv.org/abs/2602.16811v1),2026-02-18,"Charalampos Mastrokostas, Nikolaos Giarelis, Nikos Karacapilidis","Purpose: The study aims to address the research gap in evaluating the effectiveness of monolingual and multilingual Large Language Models for Greek Question Answering by introducing a novel dataset and evaluation framework. Method: The researchers developed DemosQA, a new dataset constructed from social media user questions and community-reviewed answers, and utilized a memory-efficient LLM evaluation framework to assess 11 models on 6 Greek QA datasets with 3 prompting strategies. Results: The study provides an extensive evaluation of monolingual and multilingual Large Language Models on Greek Question Answering tasks, releasing code and data to facilitate reproducibility and contributing to the advancement of language-specific QA research."
References Improve LLM Alignment in Non-Verifiable Domains,Judge Reliability And Calibration,[arXiv](http://arxiv.org/abs/2602.16802v1),2026-02-18,"Kejian Shi, Yixin Liu, Peifeng Wang, et al.","Purpose: The paper investigates whether reference-guided LLM-evaluators can bridge the gap in non-verifiable domains by serving as soft ""verifiers"" to improve LLM alignment. Method: The researchers design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs and conduct comprehensive experiments to test the effectiveness of the reference-guided approach. Results: The study shows that a reference-guided approach substantially improves the accuracy of LLM-judges, yielding clear gains over direct SFT on reference outputs and self-improvement with reference-free judges, and achieving performance comparable to training with strong finetuned reward models."
Who can we trust? LLM-as-a-jury for Comparative Assessment,"Judge Reliability And Calibration, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.16610v1),2026-02-18,"Mengjie Qian, Guangzhi Sun, Mark J. F. Gales, et al.","Purpose: The paper aims to address the issue of inconsistent and biased judgment probabilities of large language models (LLMs) when used as automatic evaluators for natural language generation assessment. Method: The authors propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Results: Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods and can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability."
CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes,"Benchmark And Dataset Creation, Metrics And Scoring Methods",[arXiv](http://arxiv.org/abs/2602.16607v1),2026-02-18,"Miguel Marques, Ana Luísa Fernandes, Ana Filipa Pacheco, et al.","Purpose: The purpose of this paper is to address the challenge of navigating lengthy and dense municipal meeting minutes by developing an automatic summarization system for European Portuguese. Method: Methodologically, the authors created a new corpus called CitiLink-Summ, which comprises 100 documents and 2,322 manually crafted summaries, and employed state-of-the-art generative models and large language models to establish baseline results. Results: Results of the study provide the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts through the evaluation with lexical and semantic metrics."
Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents,"Judge Reliability And Calibration, Benchmark And Dataset Creation",[arXiv](http://arxiv.org/abs/2602.16246v1),2026-02-18,"Yun-Shiuan Chuang, Chaitanya Kulkarni, Alec Chiu, et al.","Purpose: The paper proposes a novel evaluation framework for multi-turn tool-calling large language model agents, aiming to provide a scalable and reliable method for comparing models and generating training data. Method: The proposed approach, called Proxy State-Based Evaluation, utilizes a large language model-driven simulation framework that infers a structured proxy state from the interaction trace and verifies goal completion against scenario constraints. Results: The empirical results show that the benchmark produces stable rankings, provides effective supervision for unseen scenarios, and achieves high human-LLM judge agreement, indicating a reliable and scalable alternative to deterministic agentic benchmarks."
LiveClin: A Live Clinical Benchmark without Leakage,"Judge Reliability And Calibration, Domain-Specific Judging",[arXiv](http://arxiv.org/abs/2602.16747v1),2026-02-18,"Xidong Wang, Shuqi Guo, Yue Shen, et al.","Purpose: The paper introduces LiveClin, a live clinical benchmark designed to address the challenges of data contamination and knowledge obsolescence in medical language model evaluation. Method: LiveClin is built from contemporary, peer-reviewed case reports and updated biannually, using a verified AI-human workflow involving physicians to transform authentic patient cases into complex evaluation scenarios. Results: The evaluation of 26 models on LiveClin reveals the difficulty of real-world clinical scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%, and human experts, particularly Chief Physicians, surpassing most models in accuracy."
