# LLM as a Judge â€” Daily Report

Report date: 2026-02-06 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 6

## Papers

### PrefIx: Understand and Adapt to User Preference in Human-Agent Interaction (2602.06714v1)

Authors: Jialin Li, Zhenhao Chen, Hanjun Luo, et al.
Published: 2026-02-06
Updated: 2026-02-06
Categories: cs.HC
Link: [arXiv](http://arxiv.org/abs/2602.06714v1)

Abstract: LLM-based agents can complete tasks correctly yet still frustrate users through poor interaction patterns, such as excessive confirmations, opaque reasoning, or misaligned pacing. Current benchmarks evaluate task accuracy but overlook how agents interact: whether they infer preferences from implicit cues, adapt dynamically, or maintain fine-grained interaction quality. We introduce Prefix, a configurable environment that evaluates both what agents accomplish and how they interact. Central to Prefix is the Interaction-as-a-Tool (IaaT) paradigm, which treats interaction behaviors as structured tool calls, unifying them with existing evaluation frameworks. We define 31 preference settings across 14 attributes and formalize user experience (UX) as a core metric alongside task accuracy. A composite LLM-as-a-Judge mechanism across seven UX dimensions achieves strong aggregate reliability (ICC > 0.79), high internal consistency (alpha = 0.943), and human correlation (rho = 0.52-0.78). Preference-aware agents show 7.6% average UX improvement and 18.5% gain in preference alignment. Our work is openly accessible.

Summary:
Purpose: The paper introduces PrefIx, a configurable environment that evaluates both the task accuracy and interaction quality of human-agent interactions, addressing the limitation of current benchmarks that only focus on task completion.
Method: The approach is based on the Interaction-as-a-Tool paradigm, which treats interaction behaviors as structured tool calls, and utilizes a composite LLM-as-a-Judge mechanism to assess user experience across seven dimensions.
Results: The evaluation shows that preference-aware agents achieve an average 7.6% improvement in user experience and 18.5% gain in preference alignment, with the proposed method demonstrating strong reliability, internal consistency, and human correlation.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge (2602.06625v1)

Authors: Bo Yang, Lanfei Feng, Yunkui Chen, et al.
Published: 2026-02-06
Updated: 2026-02-06
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.06625v1)

Abstract: Existing LLM-as-a-Judge systems suffer from three fundamental limitations: limited adaptivity to task- and domain-specific evaluation criteria, systematic biases driven by non-semantic cues such as position, length, format, and model provenance, and evaluation inconsistency that leads to contradictory judgments across different evaluation modes (e.g., pointwise versus pairwise). To address these issues, we propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge. Unlike prior approaches that treat the judge as a static evaluator, FairJudge models judging behavior itself as a learnable and regularized policy. From a data-centric perspective, we construct a high-information-density judging dataset that explicitly injects supervision signals aligned with evaluation behavior. Building on this dataset, we adopt a curriculum-style SFT-DPO-GRPO training paradigm that progressively aligns rubric adherence, bias mitigation, and cross-mode consistency, while avoiding catastrophic forgetting. Experimental results on multiple internal and public benchmarks show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs. All resources will be publicly released after acceptance to facilitate future research.

Summary:
Purpose: The purpose of the paper is to propose FairJudge, an adaptive, debiased, and consistent LLM-as-a-Judge system that addresses the limitations of existing systems, including limited adaptivity, systematic biases, and evaluation inconsistency.
Method: Method: The method used to develop FairJudge involves constructing a high-information-density judging dataset and adopting a curriculum-style SFT-DPO-GRPO training paradigm that aligns rubric adherence, bias mitigation, and cross-mode consistency.
Results: Results: The results of the experiment show that FairJudge consistently improves agreement and F1, reduces non-semantic biases, and outperforms substantially larger instruction-tuned LLMs on multiple internal and public benchmarks.
Tags: Judge Prompting Protocols, Judge Reliability And Calibration

### JADE: Expert-Grounded Dynamic Evaluation for Open-Ended Professional Tasks (2602.06486v1)

Authors: Lanbo Lin, Jiayao Liu, Tianyuan Yang, et al.
Published: 2026-02-06
Updated: 2026-02-06
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.06486v1)

Abstract: Evaluating agentic AI on open-ended professional tasks faces a fundamental dilemma between rigor and flexibility. Static rubrics provide rigorous, reproducible assessment but fail to accommodate diverse valid response strategies, while LLM-as-a-judge approaches adapt to individual responses yet suffer from instability and bias. Human experts address this dilemma by combining domain-grounded principles with dynamic, claim-level assessment. Inspired by this process, we propose JADE, a two-layer evaluation framework. Layer 1 encodes expert knowledge as a predefined set of evaluation skills, providing stable evaluation criteria. Layer 2 performs report-specific, claim-level evaluation to flexibly assess diverse reasoning strategies, with evidence-dependency gating to invalidate conclusions built on refuted claims. Experiments on BizBench show that JADE improves evaluation stability and reveals critical agent failure modes missed by holistic LLM-based evaluators. We further demonstrate strong alignment with expert-authored rubrics and effective transfer to a medical-domain benchmark, validating JADE across professional domains. Our code is publicly available at https://github.com/smiling-world/JADE.

Summary:
Purpose: The paper proposes JADE, a two-layer evaluation framework, to address the dilemma between rigor and flexibility in evaluating agentic AI on open-ended professional tasks.
Method: The framework combines expert knowledge with dynamic, claim-level assessment, using a predefined set of evaluation skills and report-specific evaluation with evidence-dependency gating.
Results: Experiments demonstrate that JADE improves evaluation stability, reveals critical agent failure modes, and shows strong alignment with expert-authored rubrics, validating its effectiveness across professional domains.
Tags: Judge Prompting Protocols, Judge Reliability And Calibration

### CORE: Comprehensive Ontological Relation Evaluation for Large Language Models (2602.06446v1)

Authors: Satyam Dwivedi, Sanjukta Ghosh, Shivam Dwivedi, et al.
Published: 2026-02-06
Updated: 2026-02-06
Categories: cs.CL, cs.AI, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.06446v1)

Abstract: Large Language Models (LLMs) perform well on many reasoning benchmarks, yet existing evaluations rarely assess their ability to distinguish between meaningful semantic relations and genuine unrelatedness. We introduce CORE (Comprehensive Ontological Relation Evaluation), a dataset of 225K multiple-choice questions spanning 74 disciplines, together with a general-domain open-source benchmark of 203 rigorously validated questions (Cohen's Kappa = 1.0) covering 24 semantic relation types with equal representation of unrelated pairs. A human baseline from 1,000+ participants achieves 92.6% accuracy (95.1% on unrelated pairs). In contrast, 29 state-of-the-art LLMs achieve 48.25-70.9% overall accuracy, with near-ceiling performance on related pairs (86.5-100%) but severe degradation on unrelated pairs (0-41.35%), despite assigning similar confidence (92-94%). Expected Calibration Error increases 2-4x on unrelated pairs, and a mean semantic collapse rate of 37.6% indicates systematic generation of spurious relations. On the CORE 225K MCQs dataset, accuracy further drops to approximately 2%, highlighting substantial challenges in domain-specific semantic reasoning. We identify unrelatedness reasoning as a critical, under-evaluated frontier for LLM evaluation and safety.

Summary:
Purpose: The paper introduces CORE, a comprehensive framework for evaluating the ability of Large Language Models (LLMs) to distinguish between meaningful semantic relations and genuine unrelatedness across various disciplines.
Method: The authors created a dataset of 225K multiple-choice questions and a benchmark of 203 rigorously validated questions covering 24 semantic relation types, which were used to assess the performance of human participants and 29 state-of-the-art LLMs.
Results: The evaluation revealed that while LLMs perform well on related pairs, they struggle with unrelated pairs, achieving significantly lower accuracy and exhibiting systematic generation of spurious relations, highlighting substantial challenges in domain-specific semantic reasoning.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs (2602.07080v1)

Authors: Yicheng He, Zheng Zhao, Zhou Kaiyu, et al.
Published: 2026-02-06
Updated: 2026-02-06
Categories: cs.SE, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.07080v1)

Abstract: Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.

Summary:
Purpose: The primary objective of this research is to investigate whether a large language model's (LLM) neural dynamics encode internally decodable signals that are predictive of logical validity during code generation.
Method: This is achieved by treating code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs, and decomposing complex residual flows to identify structural signatures of sound reasoning.
Results: The analysis confirms that intrinsic correctness signals are robust across diverse syntaxes, and topological features from internal graphs predict correctness more reliably than surface heuristics, enabling targeted causal interventions to fix erroneous logic.
Tags: Unclassified

### Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math (2602.06291v1)

Authors: Guijin Son, Donghun Yang, Hitesh Laxmichand Patel, et al.
Published: 2026-02-06
Updated: 2026-02-06
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.06291v1)

Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

Summary:
Purpose: The paper aims to address the challenge of verifying research-level math solutions without relying on expert judgment by proposing a consequence-based approach for oracle-free evaluation.
Method: The proposed Consequence-Based Utility evaluator scores each candidate solution by testing its value as an in-context exemplar in solving related yet verifiable questions, building on the idea that a meaningful solution should yield better downstream performance than incorrect ones.
Results: The approach consistently outperforms existing models, including reward models and LLM judges, achieving significant improvements in accuracy and AUC scores on research-level math problems with both GPT-OSS-120B and GPT-OSS-20B models.
Tags: Metrics And Scoring Methods
