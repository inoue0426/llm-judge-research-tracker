# LLM as a Judge â€” Daily Report

Report date: 2026-02-15 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 3

## Papers

### Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity (2602.14130v1)

Authors: Kazuo Yano, Jonghyeok Lee, Tae Ishitomi, et al.
Published: 2026-02-15
Updated: 2026-02-15
Categories: cs.AI, cs.CL, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.14130v1)

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

Summary:
Purpose: This paper proposes Algebraic Quantum Intelligence (AQI) as a new framework to overcome the limitations of large language models in generating genuinely creative outputs by enabling systematic expansion of semantic space.
Method: The AQI framework is formulated as a noncommutative algebraic structure inspired by quantum theory, and its implementation involves extending a transformer-based language model with specialized operators to represent semantic states and their evolution.
Results: The evaluation of the AQI system on creative reasoning benchmarks shows that it consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance, demonstrating its potential as a practical foundation for machine creativity.
Tags: Benchmark And Dataset Creation

### Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric (2602.14069v1)

Authors: Ruipeng Jia, Yunyi Yang, Yuxin Wu, et al.
Published: 2026-02-15
Updated: 2026-02-15
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.14069v1)

Abstract: Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.

Summary:
Purpose: The paper aims to address the limitations of scalar reward models in reinforcement learning by introducing an open rubric system that scales reinforcement learning with pairwise adaptive rubric.
Method: The Open Rubric System (OpenRS) is presented as a plug-and-play framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components.
Results: The OpenRS system instantiates adaptive rubrics on the fly, performs criterion-wise pairwise comparisons, and aggregates criterion-level preferences externally, ultimately improving discriminability in open-ended settings and avoiding pointwise weighted scalarization.
Tags: Judge Prompting Protocols, Judge Reliability And Calibration

### From SFT to RL: Demystifying the Post-Training Pipeline for LLM-based Vulnerability Detection (2602.14012v1)

Authors: Youpeng Li, Fuxun Yu, Xinda Wang
Published: 2026-02-15
Updated: 2026-02-15
Categories: cs.CR, cs.AI, cs.SE
Link: [arXiv](http://arxiv.org/abs/2602.14012v1)

Abstract: The integration of LLMs into vulnerability detection (VD) has shifted the field toward interpretable and context-aware analysis. While post-training methods have shown promise in general coding tasks, their systematic application to VD remains underexplored. In this paper, we present the first comprehensive investigation into the post-training pipeline for LLM-based VD, spanning from cold-start SFT to off-policy preference optimization and on-policy RL, uncovering how data curation, stage interactions, reward mechanisms, and evaluation protocols collectively dictate the efficacy of model training and assessment. Our study identifies practical guidelines and insights: (1) SFT based on rejection sampling greatly outperforms rationalization-based supervision, which can introduce hallucinations due to ground-truth leakage. (2) While increased SFT epochs constantly benefit preference optimization, excessive SFT inhibits self-exploration during RL, ultimately limiting performance gains. (3) Coarse-grained reward signals often mislead RL, whereas fine-grained root-cause judgments ensure reliable credit assignment. Specification-based rewards offer further benefits but incur significant effort in specification generation. (4) Although filtering extremely hard-to-detect vulnerability samples improves RL training efficiency, the cost of performance loss should be considered in practical applications. (5) Models trained under GRPO significantly outperform those using SFT and preference optimization (i.e., DPO and ORPO), as well as a series of zero-shot SOTA LLMs, underscoring the significant potential of on-policy RL for LLM-based VD. (6) In contrast to binary matching that tends to overestimate performance, LLM-as-a-Judge based on root-cause analysis provides a more robust evaluation protocol, although its accuracy varies across judge models with different levels of security expertise.

Summary:
Purpose: The paper aims to investigate the post-training pipeline for Large Language Model (LLM)-based vulnerability detection, exploring the impact of various techniques on model training and assessment.
Method: The study employs a comprehensive approach, spanning from cold-start Supervised Fine-Tuning (SFT) to off-policy preference optimization and on-policy Reinforcement Learning (RL), to analyze the effects of data curation, stage interactions, reward mechanisms, and evaluation protocols.
Results: The investigation yields several key findings, including the effectiveness of SFT based on rejection sampling, the importance of fine-grained reward signals, and the superior performance of models trained under on-policy RL, as well as the need for robust evaluation protocols such as LLM-as-a-Judge.
Tags: Domain-Specific Judging, Metrics And Scoring Methods
