# LLM as a Judge â€” Daily Report

Report date: 2026-02-08 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 3

## Papers

### DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries (2602.08149v1)

Authors: Sahana Ramnath, Nima Chitsazan, Mingyang Zhou, et al.
Published: 2026-02-08
Updated: 2026-02-08
Categories: cs.CL, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.08149v1)

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

Summary:
Purpose: The paper introduces a framework called DIAL-SUMMER to evaluate dialogue summaries by addressing the complexities of structure and narration viewpoint shifts that occur when converting multiple-speaker dialogues into summarized sentences.
Method: The framework proposes a taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels, and presents a dataset composed of manually annotated dialogue summaries with fine-grained errors, which is then used for empirical analyses and experiments.
Results: The experiments demonstrate the challenging nature of the dataset, the robustness of the proposed taxonomy, and the need for future work to enhance language models' performance in detecting errors in dialogue summaries.
Tags: Benchmark And Dataset Creation, Robustness And Sensitivity

### Efficient and Adaptable Detection of Malicious LLM Prompts via Bootstrap Aggregation (2602.08062v1)

Authors: Shayan Ali Hassan, Tao Ni, Zafar Ayyub Qazi, et al.
Published: 2026-02-08
Updated: 2026-02-08
Categories: cs.LG, cs.CR
Link: [arXiv](http://arxiv.org/abs/2602.08062v1)

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and generation. However, these systems remain susceptible to malicious prompts that induce unsafe or policy-violating behavior through harmful requests, jailbreak techniques, and prompt injection attacks. Existing defenses face fundamental limitations: black-box moderation APIs offer limited transparency and adapt poorly to evolving threats, while white-box approaches using large LLM judges impose prohibitive computational costs and require expensive retraining for new attacks. Current systems force designers to choose between performance, efficiency, and adaptability. To address these challenges, we present BAGEL (Bootstrap AGgregated Ensemble Layer), a modular, lightweight, and incrementally updatable framework for malicious prompt detection. BAGEL employs a bootstrap aggregation and mixture of expert inspired ensemble of fine-tuned models, each specialized on a different attack dataset. At inference, BAGEL uses a random forest router to identify the most suitable ensemble member, then applies stochastic selection to sample additional members for prediction aggregation. When new attacks emerge, BAGEL updates incrementally by fine-tuning a small prompt-safety classifier (86M parameters) and adding the resulting model to the ensemble. BAGEL achieves an F1 score of 0.92 by selecting just 5 ensemble members (430M parameters), outperforming OpenAI Moderation API and ShieldGemma which require billions of parameters. Performance remains robust after nine incremental updates, and BAGEL provides interpretability through its router's structural features. Our results show ensembles of small finetuned classifiers can match or exceed billion-parameter guardrails while offering the adaptability and efficiency required for production systems.

Summary:
Purpose: The paper aims to address the challenges of detecting malicious prompts in Large Language Models (LLMs) by presenting a modular and efficient framework called BAGEL.
Method: The BAGEL framework employs a bootstrap aggregation and mixture of expert-inspired ensemble of fine-tuned models, each specialized on a different attack dataset, and uses a random forest router for inference.
Results: The proposed BAGEL framework achieves an F1 score of 0.92 and outperforms existing methods, including OpenAI Moderation API and ShieldGemma, while providing adaptability, efficiency, and interpretability.
Tags: Benchmark And Dataset Creation, Domain-Specific Judging

### The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation (2602.07996v1)

Authors: Arash Marioriyad, Omid Ghahroodi, Ehsaneddin Asgari, et al.
Published: 2026-02-08
Updated: 2026-02-08
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.07996v1)

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

Summary:
Purpose: The paper aims to test the ideal of a faithful judge in large language models (LLMs) used for automatic evaluation in tasks such as reasoning and creative writing.
Method: The researchers conduct experiments using controlled cue perturbations, injecting synthetic metadata labels into evaluation prompts, across six judge models and two datasets with distinct evaluation regimes.
Results: The study finds that LLM-based judges often rely on hidden shortcuts, such as provenance hierarchies and recency preferences, which are not explicitly acknowledged in their rationales, revealing an explanation gap in model-based evaluation pipelines.
Tags: Judge Reliability And Calibration, Robustness And Sensitivity
