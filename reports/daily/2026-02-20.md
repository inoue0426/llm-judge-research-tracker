# LLM as a Judge — Daily Report

Report date: 2026-02-20 (UTC)
Coverage window: last 3 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 13

## Papers

### Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models (2602.17433v1)

Authors: Francesco Ortu, Joeun Yook, Punya Syon Pandey, et al.
Published: 2026-02-19
Updated: 2026-02-19
Categories: cs.CY
Link: [arXiv](http://arxiv.org/abs/2602.17433v1)

Abstract: Large language models (LLMs) are increasingly used as sources of historical information, motivating the need for scalable audits on contested events and politically charged narratives in settings that mirror real user interactions. We introduce \textsc{\texttt{HistoricalMisinfo}}, a curated dataset of $500$ contested events from $45$ countries, each paired with a factual reference narrative and a documented revisionist reference narrative. To approximate real-world usage, we instantiate each event in $11$ prompt scenarios that reflect common communication settings (e.g., questions, textbooks, social posts, policy briefs). Using an LLM-as-a-judge protocol that compares model outputs to the two references, we evaluate LLMs varying across model architectures in two conditions: (i) neutral user prompts that ask for factually accurate information, and (ii) robustness prompts in which the user explicitly requests the revisionist version of the event. Under neutral prompts, models are generally closer to factual references, though the resulting scores should be interpreted as reference-alignment signals rather than definitive evidence of human-interpretable revisionism. Robustness prompting yields a strong and consistent effect: when the user requests the revisionist narrative, all evaluated models show sharply higher revisionism scores, indicating limited resistance or self-correction. \textsc{\texttt{HistoricalMisinfo}} provides a practical foundation for benchmarking robustness to revisionist framing and for guiding future work on more precise automatic evaluation of contested historical claims to ensure a sustainable integration of AI systems within society.

Summary:
Purpose: The purpose of this study is to introduce a method for detecting historical revisionism in large language models, which are increasingly used as sources of historical information.
Method: The method involves using a curated dataset called HistoricalMisinfo, which contains 500 contested events from 45 countries, each paired with factual and revisionist reference narratives, and evaluating language models under neutral and robustness prompts.
Results: The results show that while language models generally produce outputs closer to factual references under neutral prompts, they exhibit sharply higher revisionism scores when explicitly requested to provide the revisionist narrative, indicating limited resistance to revisionist framing.
Tags: Benchmark And Dataset Creation, Robustness And Sensitivity

### Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation (2602.17316v1)

Authors: Bogdan Kostić, Conor Fallon, Julian Risch, et al.
Published: 2026-02-19
Updated: 2026-02-19
Categories: cs.CL, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.17316v1)

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

Summary:
Purpose: The paper examines how controlled lexical and syntactic perturbations affect the performance and relative ranking of Large Language Models (LLMs) across various benchmarks.
Method: The study employs two linguistically principled pipelines to generate meaning-preserving variations, including synonym substitution for lexical changes and dependency parsing for syntactic transformations.
Results: The results show that lexical perturbations induce substantial performance degradation, while syntactic perturbations have heterogeneous effects, and both types destabilize model leaderboards, revealing a need for robustness testing in LLM evaluation.
Tags: Robustness And Sensitivity, Metrics And Scoring Methods

### On the Reliability of User-Centric Evaluation of Conversational Recommender Systems (2602.17264v1)

Authors: Michael Müller, Amir Reza Mohammadi, Andreas Peintner, et al.
Published: 2026-02-19
Updated: 2026-02-19
Categories: cs.IR
Link: [arXiv](http://arxiv.org/abs/2602.17264v1)

Abstract: User-centric evaluation has become a key paradigm for assessing Conversational Recommender Systems (CRS), aiming to capture subjective qualities such as satisfaction, trust, and rapport. To enable scalable evaluation, recent work increasingly relies on third-party annotations of static dialogue logs by crowd workers or large language models. However, the reliability of this practice remains largely unexamined. In this paper, we present a large-scale empirical study investigating the reliability and structure of user-centric CRS evaluation on static dialogue transcripts. We collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework. Using random-effects reliability models and correlation analysis, we quantify the stability of individual dimensions and their interdependencies. Our results show that utilitarian and outcome-oriented dimensions such as accuracy, usefulness, and satisfaction achieve moderate reliability under aggregation, whereas socially grounded constructs such as humanness and rapport are substantially less reliable. Furthermore, many dimensions collapse into a single global quality signal, revealing a strong halo effect in third-party judgments. These findings challenge the validity of single-annotator and LLM-based evaluation protocols and motivate the need for multi-rater aggregation and dimension reduction in offline CRS evaluation.

Summary:
Purpose: The paper aims to investigate the reliability of user-centric evaluation of Conversational Recommender Systems (CRS) through a large-scale empirical study on static dialogue transcripts.
Method: The study collected 1,053 annotations from 124 crowd workers on 200 ReDial dialogues using the 18-dimensional CRS-Que framework and analyzed them using random-effects reliability models and correlation analysis.
Results: The results show that while utilitarian dimensions achieve moderate reliability, socially grounded constructs are less reliable, and many dimensions collapse into a single global quality signal due to a strong halo effect in third-party judgments.
Tags: Judge Reliability And Calibration

### Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study (2602.17262v1)

Authors: Kensuke Okada, Yui Furukawa, Kyosuke Bunji
Published: 2026-02-19
Updated: 2026-02-19
Categories: cs.CL, stat.ME
Link: [arXiv](http://arxiv.org/abs/2602.17262v1)

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

Summary:
Purpose: The study aims to quantify and mitigate socially desirable responding in large language models (LLMs) when using human self-report questionnaires for evaluation, as these instruments presume honest responding but can be biased by socially preferred answers.
Method: The researchers propose a psychometric framework that administers the same inventory under HONEST versus FAKE-GOOD instructions to quantify socially desirable responding and construct a graded forced-choice inventory to mitigate it.
Results: The study finds that desirability-matched graded forced-choice questionnaires substantially attenuate socially desirable responding in LLMs while preserving the recovery of intended persona profiles, highlighting a model-dependent trade-off between socially desirable responding and recovery.
Tags: Judge Reliability And Calibration, Metrics And Scoring Methods

### When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment (2602.17170v1)

Authors: Chuting Yu, Hang Li, Joel Mackenzie, et al.
Published: 2026-02-19
Updated: 2026-02-19
Categories: cs.IR
Link: [arXiv](http://arxiv.org/abs/2602.17170v1)

Abstract: Human relevance assessment is time-consuming and cognitively intensive, limiting the scalability of Information Retrieval evaluation. This has led to growing interest in using large language models (LLMs) as proxies for human judges. However, it remains an open question whether LLM-based relevance judgments are reliable, stable, and rigorous enough to match humans for relevance assessment. In this work, we conduct a systematic study of overrating behavior in LLM-based relevance judgments across model backbones, evaluation paradigms (pointwise and pairwise), and passage modification strategies. We show that models consistently assign inflated relevance scores -- often with high confidence -- to passages that do not genuinely satisfy the underlying information need, revealing a system-wide bias rather than random fluctuations in judgment. Furthermore, controlled experiments show that LLM-based relevance judgments can be highly sensitive to passage length and surface-level lexical cues. These results raise concerns about the usage of LLMs as drop-in replacements for human relevance assessors, and highlight the urgent need for careful diagnostic evaluation frameworks when applying LLMs for relevance assessments. Our code and results are publicly available.

Summary:
Purpose: The paper aims to explore the reliability and stability of large language models (LLMs) as proxies for human judges in relevance assessment, by investigating their overrating behavior across different model backbones and evaluation paradigms.
Method: The study conducts a systematic analysis of LLM-based relevance judgments using various passage modification strategies, pointwise and pairwise evaluation paradigms, and controlled experiments to examine the sensitivity of models to passage length and lexical cues.
Results: The results show that LLMs consistently assign inflated relevance scores to passages that do not genuinely satisfy the underlying information need, revealing a system-wide bias and highlighting the need for careful diagnostic evaluation frameworks when using LLMs for relevance assessments.
Tags: Judge Reliability And Calibration, Judge Prompting Protocols

### The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI (2602.17127v1)

Authors: Dusan Bosnjakovic
Published: 2026-02-19
Updated: 2026-02-19
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.17127v1)

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions. This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization. Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

Summary:
Purpose: The paper aims to introduce a novel auditing framework for detecting durable, provider-level behavioral signatures in Large Language Models (LLMs) to ensure safety and governance in multi-agent systems.
Method: The research utilizes psychometric measurement theory, specifically latent trait estimation under ordinal uncertainty, and employs techniques such as forced-choice ordinal vignettes and Mixed Linear Models (MixedLM) analysis to quantify latent tendencies in LLMs.
Results: The study finds that a persistent "lab signal" accounts for significant behavioral clustering in leading models, indicating that latent biases are compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### SourceBench: Can AI Answers Reference Quality Web Sources? (2602.16942v1)

Authors: Hexi Jin, Stephen Liu, Yuheng Li, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.16942v1)

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

Summary:
Purpose: The paper introduces SourceBench, a benchmark for measuring the quality of cited web sources used by large language models to answer queries, with the goal of evaluating evidence quality rather than just answer correctness.
Method: The SourceBench framework uses eight metrics to assess content quality and page-level signals, and includes a human-labeled dataset with a calibrated LLM-based evaluator to evaluate the quality of cited web sources across various queries and intents.
Results: The evaluation of eight large language models, Google Search, and three AI search tools using SourceBench reveals key insights that can guide future research in GenAI and web search, including the quality of cited web sources and the effectiveness of different models.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark (2602.16811v1)

Authors: Charalampos Mastrokostas, Nikolaos Giarelis, Nikos Karacapilidis
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.CL, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.16811v1)

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

Summary:
Purpose: The study aims to address the research gap in evaluating the effectiveness of monolingual and multilingual Large Language Models for Greek Question Answering by introducing a novel dataset and evaluation framework.
Method: The researchers developed DemosQA, a new dataset constructed from social media user questions and community-reviewed answers, and utilized a memory-efficient LLM evaluation framework to assess 11 models on 6 Greek QA datasets with 3 prompting strategies.
Results: The study provides an extensive evaluation of monolingual and multilingual Large Language Models on Greek Question Answering tasks, releasing code and data to facilitate reproducibility and contributing to the advancement of language-specific QA research.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### References Improve LLM Alignment in Non-Verifiable Domains (2602.16802v1)

Authors: Kejian Shi, Yixin Liu, Peifeng Wang, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.CL, cs.AI, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.16802v1)

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

Summary:
Purpose: The paper investigates whether reference-guided LLM-evaluators can bridge the gap in non-verifiable domains by serving as soft "verifiers" to improve LLM alignment.
Method: The researchers design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs and conduct comprehensive experiments to test the effectiveness of the reference-guided approach.
Results: The study shows that a reference-guided approach substantially improves the accuracy of LLM-judges, yielding clear gains over direct SFT on reference outputs and self-improvement with reference-free judges, and achieving performance comparable to training with strong finetuned reward models.
Tags: Judge Reliability And Calibration

### Who can we trust? LLM-as-a-jury for Comparative Assessment (2602.16610v1)

Authors: Mengjie Qian, Guangzhi Sun, Mark J. F. Gales, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.CL, cs.AI, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.16610v1)

Abstract: Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.

Summary:
Purpose: The paper aims to address the issue of inconsistent and biased judgment probabilities of large language models (LLMs) when used as automatic evaluators for natural language generation assessment.
Method: The authors propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone.
Results: Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods and can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes (2602.16607v1)

Authors: Miguel Marques, Ana Luísa Fernandes, Ana Filipa Pacheco, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.16607v1)

Abstract: Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.

Summary:
Purpose: The purpose of this paper is to address the challenge of navigating lengthy and dense municipal meeting minutes by developing an automatic summarization system for European Portuguese.
Method: Methodologically, the authors created a new corpus called CitiLink-Summ, which comprises 100 documents and 2,322 manually crafted summaries, and employed state-of-the-art generative models and large language models to establish baseline results.
Results: Results of the study provide the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts through the evaluation with lexical and semantic metrics.
Tags: Benchmark And Dataset Creation, Metrics And Scoring Methods

### Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents (2602.16246v1)

Authors: Yun-Shiuan Chuang, Chaitanya Kulkarni, Alec Chiu, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.16246v1)

Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.

Summary:
Purpose: The paper proposes a novel evaluation framework for multi-turn tool-calling large language model agents, aiming to provide a scalable and reliable method for comparing models and generating training data.
Method: The proposed approach, called Proxy State-Based Evaluation, utilizes a large language model-driven simulation framework that infers a structured proxy state from the interaction trace and verifies goal completion against scenario constraints.
Results: The empirical results show that the benchmark produces stable rankings, provides effective supervision for unseen scenarios, and achieves high human-LLM judge agreement, indicating a reliable and scalable alternative to deterministic agentic benchmarks.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### LiveClin: A Live Clinical Benchmark without Leakage (2602.16747v1)

Authors: Xidong Wang, Shuqi Guo, Yue Shen, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.LG, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.16747v1)

Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.

Summary:
Purpose: The paper introduces LiveClin, a live clinical benchmark designed to address the challenges of data contamination and knowledge obsolescence in medical language model evaluation.
Method: LiveClin is built from contemporary, peer-reviewed case reports and updated biannually, using a verified AI-human workflow involving physicians to transform authentic patient cases into complex evaluation scenarios.
Results: The evaluation of 26 models on LiveClin reveals the difficulty of real-world clinical scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%, and human experts, particularly Chief Physicians, surpassing most models in accuracy.
Tags: Judge Reliability And Calibration, Domain-Specific Judging
