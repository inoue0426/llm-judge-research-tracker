# LLM as a Judge â€” Daily Report

Report date: 2026-02-14 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 3

## Papers

### T2MBench: A Benchmark for Out-of-Distribution Text-to-Motion Generation (2602.13751v1)

Authors: Bin Yang, Rong Ou, Weisheng Xu, et al.
Published: 2026-02-14
Updated: 2026-02-14
Categories: cs.CV
Link: [arXiv](http://arxiv.org/abs/2602.13751v1)

Abstract: Most existing evaluations of text-to-motion generation focus on in-distribution textual inputs and a limited set of evaluation criteria, which restricts their ability to systematically assess model generalization and motion generation capabilities under complex out-of-distribution (OOD) textual conditions. To address this limitation, we propose a benchmark specifically designed for OOD text-to-motion evaluation, which includes a comprehensive analysis of 14 representative baseline models and the two datasets derived from evaluation results. Specifically, we construct an OOD prompt dataset consisting of 1,025 textual descriptions. Based on this prompt dataset, we introduce a unified evaluation framework that integrates LLM-based Evaluation, Multi-factor Motion evaluation, and Fine-grained Accuracy Evaluation. Our experimental results reveal that while different baseline models demonstrate strengths in areas such as text-to-motion semantic alignment, motion generalizability, and physical quality, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation. These findings highlight the limitations of existing methods in OOD scenarios and offer practical guidance for the design and evaluation of future production-level text-to-motion models.

Summary:
Purpose: The paper proposes a benchmark specifically designed for out-of-distribution text-to-motion evaluation to address the limitation of existing evaluations that focus on in-distribution textual inputs.
Method: The benchmark includes a comprehensive analysis of 14 representative baseline models and two datasets derived from evaluation results, as well as a unified evaluation framework that integrates multiple evaluation methods.
Results: The experimental results reveal that while different baseline models demonstrate strengths in certain areas, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation, highlighting the limitations of existing methods in out-of-distribution scenarios.
Tags: Benchmark And Dataset Creation, Judge Prompting Protocols

### Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges (2602.13576v1)

Authors: Ruomeng Ding, Yifei Pang, He Sun, et al.
Published: 2026-02-14
Updated: 2026-02-14
Categories: cs.CR, cs.AI, cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.13576v1)

Abstract: Evaluation and alignment pipelines for large language models increasingly rely on LLM-based judges, whose behavior is guided by natural-language rubrics and validated on benchmarks. We identify a previously under-recognized vulnerability in this workflow, which we term Rubric-Induced Preference Drift (RIPD). Even when rubric edits pass benchmark validation, they can still produce systematic and directional shifts in a judge's preferences on target domains. Because rubrics serve as a high-level decision interface, such drift can emerge from seemingly natural, criterion-preserving edits and remain difficult to detect through aggregate benchmark metrics or limited spot-checking. We further show this vulnerability can be exploited through rubric-based preference attacks, in which benchmark-compliant rubric edits steer judgments away from a fixed human or trusted reference on target domains, systematically inducing RIPD and reducing target-domain accuracy up to 9.5% (helpfulness) and 27.9% (harmlessness). When these judgments are used to generate preference labels for downstream post-training, the induced bias propagates through alignment pipelines and becomes internalized in trained policies. This leads to persistent and systematic drift in model behavior. Overall, our findings highlight evaluation rubrics as a sensitive and manipulable control interface, revealing a system-level alignment risk that extends beyond evaluator reliability alone. The code is available at: https://github.com/ZDCSlab/Rubrics-as-an-Attack-Surface. Warning: Certain sections may contain potentially harmful content that may not be appropriate for all readers.

Summary:
Purpose: The paper aims to identify and explore a previously under-recognized vulnerability in evaluation and alignment pipelines for large language models, which relies on natural-language rubrics and validated benchmarks.
Method: The researchers analyze the behavior of LLM-based judges guided by these rubrics and investigate how even minor edits to the rubrics can produce systematic shifts in a judge's preferences on target domains.
Results: The study reveals that rubric-induced preference drift can lead to significant reductions in target-domain accuracy, propagating bias through alignment pipelines and resulting in persistent drift in model behavior, with accuracy decreases of up to 9.5% and 27.9% in certain cases.
Tags: Judge Reliability And Calibration, Judge Prompting Protocols

### Small Reward Models via Backward Inference (2602.13551v1)

Authors: Yike Wang, Faeze Brahman, Shangbin Feng, et al.
Published: 2026-02-14
Updated: 2026-02-14
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.13551v1)

Abstract: Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.

Summary:
Purpose: The purpose of this work is to propose a new approach to reward modeling called FLIP, which reformulates reward modeling through backward inference to improve flexibility and accessibility in non-verifiable domains.
Method: Methodologically, the authors evaluate FLIP across four domains using 13 small language models, comparing its performance to LLM-as-a-Judge baselines and assessing its effectiveness in various scenarios.
Results: Results show that FLIP outperforms baseline methods by an average of 79.6%, substantially improves downstream performance, and is particularly effective for longer outputs and robust to common forms of reward hacking.
Tags: Judge Prompting Protocols
