# LLM as a Judge â€” Daily Report

Report date: 2026-02-16 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 4

## Papers

### Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation (2602.14564v1)

Authors: Shefayat E Shams Adib, Ahmed Alfey Sani, Ekramul Alam Esham, et al.
Published: 2026-02-16
Updated: 2026-02-16
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.14564v1)

Abstract: Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.

Summary:
Purpose: The paper aims to compare the performance of five Large Language Models (LLMs) for medical question answering (QA) using a zero-shot evaluation methodology on the iCliniq dataset.
Method: The comparison is done by deploying the LLMs and evaluating their performance using BLEU and ROUGE metrics without specialized fine-tuning, considering models such as Llama-3-8B-Instruct and GPT-5-mini.
Results: The results show that larger models like Llama 3.3 70B Instruct outperform smaller models, with Llama-4-Maverick-17B exhibiting competitive results, highlighting the trade-offs between model size and performance in medical QA applications.
Tags: Benchmark And Dataset Creation, Domain-Specific Judging

### Synthetic Reader Panels: Tournament-Based Ideation with LLM Personas for Autonomous Publishing (2602.14433v1)

Authors: Fred Zimmerman
Published: 2026-02-16
Updated: 2026-02-16
Categories: cs.CY, cs.AI, cs.CL, cs.HC
Link: [arXiv](http://arxiv.org/abs/2602.14433v1)

Abstract: We present a system for autonomous book ideation that replaces human focus groups with synthetic reader panels -- diverse collections of LLM-instantiated reader personas that evaluate book concepts through structured tournament competitions. Each persona is defined by demographic attributes (age group, gender, income, education, reading level), behavioral patterns (books per year, genre preferences, discovery methods, price sensitivity), and consistency parameters. Panels are composed per imprint to reflect target demographics, with diversity constraints ensuring representation across age, reading level, and genre affinity. Book concepts compete in single-elimination, double-elimination, round-robin, or Swiss-system tournaments, judged against weighted criteria including market appeal, originality, and execution potential. To reject low-quality LLM evaluations, we implement five automated anti-slop checks (repetitive phrasing, generic framing, circular reasoning, score clustering, audience mismatch). We report results from deployment within a multi-imprint publishing operation managing 6 active imprints and 609 titles in distribution. Three case studies -- a 270-evaluator panel for a children's literacy novel, and two 5-person expert panels for a military memoir and a naval strategy monograph -- demonstrate that synthetic panels produce actionable demographic segmentation, identify structural content issues invisible to homogeneous reviewers, and enable tournament filtering that eliminates low-quality concepts while enriching high-quality survivors from 15% to 62% of the evaluated pool.

Summary:
Purpose: The purpose of this paper is to present a system for autonomous book ideation that utilizes synthetic reader panels composed of diverse LLM-instantiated reader personas to evaluate book concepts through structured tournament competitions.
Method: The method involves creating panels that reflect target demographics with diversity constraints, and using these panels to compete book concepts in various tournament formats, with evaluations judged against weighted criteria and filtered through automated anti-slop checks.
Results: The results show that synthetic reader panels can produce actionable demographic segmentation, identify structural content issues, and effectively filter out low-quality book concepts while enriching high-quality ones, as demonstrated through three case studies within a multi-imprint publishing operation.
Tags: Judge Reliability And Calibration, Judge Prompting Protocols

### InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem (2602.14367v1)

Authors: Shuofei Qiao, Yunxiang Wei, Xuehai Wang, et al.
Published: 2026-02-16
Updated: 2026-02-16
Categories: cs.CL, cs.AI, cs.IR, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.14367v1)

Abstract: The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.

Summary:
Purpose: The paper introduces InnoEval, a deep innovation evaluation framework designed to address the limitations of existing idea evaluation methods by regarding idea evaluation as a knowledge-grounded, multi-perspective reasoning problem.
Method: The approach involves applying a heterogeneous deep knowledge search engine to retrieve dynamic evidence from diverse online sources and utilizing an innovation review board with reviewers from distinct academic backgrounds for multi-dimensional evaluation.
Results: Experiments demonstrate that InnoEval consistently outperforms baselines in various evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts, as evidenced by its performance on comprehensive datasets derived from authoritative peer-reviewed submissions.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### Key Considerations for Domain Expert Involvement in LLM Design and Evaluation: An Ethnographic Study (2602.14357v1)

Authors: Annalisa Szymanski, Oghenemaro Anuyah, Toby Jia-Jun Li, et al.
Published: 2026-02-16
Updated: 2026-02-16
Categories: cs.HC, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.14357v1)

Abstract: Large Language Models (LLMs) are increasingly developed for use in complex professional domains, yet little is known about how teams design and evaluate these systems in practice. This paper examines the challenges and trade-offs in LLM development through a 12-week ethnographic study of a team building a pedagogical chatbot. The researcher observed design and evaluation activities and conducted interviews with both developers and domain experts. Analysis revealed four key practices: creating workarounds for data collection, turning to augmentation when expert input was limited, co-developing evaluation criteria with experts, and adopting hybrid expert-developer-LLM evaluation strategies. These practices show how teams made strategic decisions under constraints and demonstrate the central role of domain expertise in shaping the system. Challenges included expert motivation and trust, difficulties structuring participatory design, and questions around ownership and integration of expert knowledge. We propose design opportunities for future LLM development workflows that emphasize AI literacy, transparent consent, and frameworks recognizing evolving expert roles.

Summary:
Purpose: The purpose of this paper is to examine the challenges and trade-offs in Large Language Model (LLM) development through an ethnographic study of a team building a pedagogical chatbot.
Method: The method used in this research involves a 12-week ethnographic study where the researcher observed design and evaluation activities and conducted interviews with both developers and domain experts to identify key practices in LLM development.
Results: Results of the study revealed four key practices that demonstrate the central role of domain expertise in shaping the system, including creating workarounds for data collection and adopting hybrid expert-developer-LLM evaluation strategies, which inform design opportunities for future LLM development workflows.
Tags: Judge Prompting Protocols, Benchmark And Dataset Creation
