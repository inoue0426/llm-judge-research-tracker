# LLM as a Judge â€” Daily Report

Report date: 2026-01-31 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 6

## Papers

### MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers (2602.00933v1)

Authors: Chaithanya Bandi, Ben Hertzberg, Geobio Boo, et al.
Published: 2026-01-31
Updated: 2026-01-31
Categories: cs.SE, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.00933v1)

Abstract: The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or subjective LLM-as-a-judge metrics. We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools. It includes 1,000 tasks designed to assess tool-use competency in realistic, multi-step workflows. Tasks use natural language prompts that avoid naming specific tools or servers, requiring agents to identify and orchestrate 3-6 tool calls across multiple servers. We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the model's final answer, complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency. Evaluation results on frontier models reveal that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding. We release the task schema, containerized harness, and a 500-task public subset of the benchmark dataset to facilitate reproducible comparisons and advance the development of robust, tool-augmented agents.

Summary:
Purpose: The paper introduces MCP-Atlas, a large-scale benchmark for evaluating tool-use competency with real Model Context Protocol (MCP) servers, aiming to assess the ability of Large Language Models (LLMs) to discover and invoke external tools in realistic scenarios.
Method: The benchmark comprises 36 real MCP servers and 220 tools, with 1,000 tasks designed to evaluate tool-use competency in multi-step workflows using natural language prompts, and a claims-based rubric is used to score the tasks.
Results: The evaluation results on frontier models show that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding, demonstrating the effectiveness of the MCP-Atlas benchmark in assessing tool-use competency.
Tags: Benchmark And Dataset Creation, Metrics And Scoring Methods

### Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity (2602.00723v1)

Authors: Prakhar Ganesh, Reza Shokri, Golnoosh Farnadi
Published: 2026-01-31
Updated: 2026-01-31
Categories: cs.LG, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.00723v1)

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

Summary:
Purpose: The purpose of this paper is to reevaluate the concept of hallucinations in large language models by considering not only correctness but also consistency, which is necessary to distinguish and address the various harms caused by hallucinations.
Method: Methodologically, the authors introduce a framework called prompt multiplicity to quantify consistency in language model evaluations, allowing for a more comprehensive analysis of hallucination-related issues.
Results: Results from the analysis reveal significant inconsistencies in benchmarks, with over 50% inconsistency found, and highlight critical limitations in current detection and mitigation strategies, suggesting that hallucination-related harms have been severely misunderstood.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation (2602.00665v1)

Authors: Lakshan Cooray, Deshan Sumanathilaka, Pattigadapa Venkatesh Raju
Published: 2026-01-31
Updated: 2026-01-31
Categories: cs.CL, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.00665v1)

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

Summary:
Purpose: The study aims to investigate the effectiveness of small language models for context-summarized multi-turn customer-service question answering, exploring their ability to handle dialogue continuity and contextual understanding.
Method: The researchers evaluate nine instruction-tuned low-parameterized small language models against three commercial large language models using a combination of lexical and semantic similarity metrics, human evaluation, and LLM-as-a-judge methods.
Results: The results show significant variation across the small language models, with some demonstrating near-large language model performance, while others struggle to maintain dialogue continuity and contextual alignment, highlighting both the potential and limitations of these models.
Tags: Robustness And Sensitivity, Judge Reliability And Calibration

### Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction (2602.00575v1)

Authors: Chaoqun Cui, Jing Huang, Shijing Wang, et al.
Published: 2026-01-31
Updated: 2026-01-31
Categories: cs.RO
Link: [arXiv](http://arxiv.org/abs/2602.00575v1)

Abstract: Reinforcement learning with verifiable rewards (RLVR) is pivotal for the continuous evolution of GUI agents, yet existing evaluation paradigms face significant limitations. Rule-based methods suffer from poor scalability and cannot handle open-ended tasks, while LLM-as-a-Judge approaches rely on passive visual observation, often failing to capture latent system states due to partial state observability. To address these challenges, we advocate for a paradigm shift from passive evaluation to Agentic Interactive Verification. We introduce VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion. Leveraging the insight that GUI tasks are typically "easy to verify but hard to solve", VAGEN overcomes the bottlenecks of visual limitations. Experimental results on OSWorld-Verified and AndroidWorld benchmarks demonstrate that VAGEN significantly improves evaluation accuracy compared to LLM-as-a-Judge baselines and further enhances performance through test-time scaling strategies.

Summary:
Purpose: The paper aims to address the limitations of existing evaluation paradigms for GUI agents, such as poor scalability and inability to handle open-ended tasks, by introducing a new paradigm called Agentic Interactive Verification.
Method: The authors propose VAGEN, a framework that employs a verifier agent equipped with interaction tools to autonomously plan verification strategies and proactively probe the environment for evidence of task completion.
Results: Experimental results demonstrate that VAGEN significantly improves evaluation accuracy compared to existing baselines and enhances performance through test-time scaling strategies on OSWorld-Verified and AndroidWorld benchmarks.
Tags: Benchmark And Dataset Creation

### Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory (2602.00521v1)

Authors: Junhyuk Choi, Sohhyung Park, Chanhee Cho, et al.
Published: 2026-01-31
Updated: 2026-01-31
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.00521v1)

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

Summary:
Purpose: The purpose of this study is to introduce a diagnostic framework for assessing the reliability of Large Language Models (LLMs) used as judges in automated evaluation, addressing the limitation of existing validation practices that only examine observed outputs.
Method: The method employed in this study utilizes Item Response Theory (IRT), specifically the Graded Response Model (GRM), to formalize reliability along two dimensions: intrinsic consistency and human alignment, providing a two-phase framework for diagnosing LLM judges.
Results: The results of this study demonstrate that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically, offering practical guidance for verifying the reliability of LLM-as-a-Judge and identifying potential causes of unreliability.
Tags: Judge Reliability And Calibration

### Judging the Judges: Human Validation of Multi-LLM Evaluation for High-Quality K--12 Science Instructional Materials (2602.13243v1)

Authors: Peng He, Zhaohui Li, Zeyuan Wang, et al.
Published: 2026-01-31
Updated: 2026-01-31
Categories: cs.CY, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.13243v1)

Abstract: Designing high-quality, standards-aligned instructional materials for K--12 science is time-consuming and expertise-intensive. This study examines what human experts notice when reviewing AI-generated evaluations of such materials, aiming to translate their insights into design principles for a future GenAI-based instructional material design agent. We intentionally selected 12 high-quality curriculum units across life, physical, and earth sciences from validated programs such as OpenSciEd and Multiple Literacies in Project-based Learning. Using the EQuIP rubric with 9 evaluation items, we prompted GPT-4o, Claude, and Gemini to produce numerical ratings and written rationales for each unit, generating 648 evaluation outputs. Two science education experts independently reviewed all outputs, marking agreement (1) or disagreement (0) for both scores and rationales, and offering qualitative reflections on AI reasoning. This process surfaces patterns in where LLM judgments align with or diverge from expert perspectives, revealing reasoning strengths, gaps, and contextual nuances. These insights will directly inform the development of a domain-specific GenAI agent to support the design of high-quality instructional materials in K--12 science education.

Summary:
Purpose: The study aims to examine what human experts notice when reviewing AI-generated evaluations of high-quality K-12 science instructional materials, with the goal of translating their insights into design principles for a future GenAI-based instructional material design agent.
Method: The researchers used the EQuIP rubric to prompt three large language models to produce numerical ratings and written rationales for 12 high-quality curriculum units, which were then reviewed by two science education experts who marked agreement or disagreement with the AI outputs and offered qualitative reflections.
Results: The study reveals patterns in where LLM judgments align with or diverge from expert perspectives, surfacing reasoning strengths, gaps, and contextual nuances that will inform the development of a domain-specific GenAI agent to support the design of high-quality instructional materials in K-12 science education.
Tags: Judge Reliability And Calibration, Judge Prompting Protocols
