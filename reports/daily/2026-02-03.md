# LLM as a Judge — Daily Report

Report date: 2026-02-03 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 6

## Papers

### No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding (2602.03709v1)

Authors: Vynska Amalia Permadi, Xingwei Tan, Nafise Sadat Moosavi, et al.
Published: 2026-02-03
Updated: 2026-02-03
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.03709v1)

Abstract: Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.

Summary:
Purpose: The paper aims to introduce a new large-scale multi-hop question answering dataset, called ID-MoCQA, for assessing the cultural understanding of large language models grounded in Indonesian traditions.
Method: The authors present a framework that transforms single-hop cultural questions into multi-hop reasoning chains and utilize a multi-stage validation pipeline combining expert review and LLM-as-a-judge filtering to ensure high-quality question-answer pairs.
Results: The evaluation of state-of-the-art models using ID-MoCQA reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference, providing a challenging benchmark for advancing the cultural competency of large language models.
Tags: Benchmark And Dataset Creation

### Can LLMs Do Rocket Science? Exploring the Limits of Complex Reasoning with GTOC 12 (2602.03630v1)

Authors: Iñaki del Campo, Pablo Cuervo, Victor Rodriguez-Fernandez, et al.
Published: 2026-02-03
Updated: 2026-02-03
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.03630v1)

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation and general reasoning, yet their capacity for autonomous multi-stage planning in high-dimensional, physically constrained environments remains an open research question. This study investigates the limits of current AI agents by evaluating them against the 12th Global Trajectory Optimization Competition (GTOC 12), a complex astrodynamics challenge requiring the design of a large-scale asteroid mining campaign. We adapt the MLE-Bench framework to the domain of orbital mechanics and deploy an AIDE-based agent architecture to autonomously generate and refine mission solutions. To assess performance beyond binary validity, we employ an "LLM-as-a-Judge" methodology, utilizing a rubric developed by domain experts to evaluate strategic viability across five structural categories. A comparative analysis of models, ranging from GPT-4-Turbo to reasoning-enhanced architectures like Gemini 2.5 Pro, and o3, reveals a significant trend: the average strategic viability score has nearly doubled in the last two years (rising from 9.3 to 17.2 out of 26). However, we identify a critical capability gap between strategy and execution. While advanced models demonstrate sophisticated conceptual understanding, correctly framing objective functions and mission architectures, they consistently fail at implementation due to physical unit inconsistencies, boundary condition errors, and inefficient debugging loops. We conclude that, while current LLMs often demonstrate sufficient knowledge and intelligence to tackle space science tasks, they remain limited by an implementation barrier, functioning as powerful domain facilitators rather than fully autonomous engineers.

Summary:
Purpose: The study investigates the limits of Large Language Models (LLMs) in autonomous multi-stage planning within complex, physically constrained environments, such as astrodynamics.
Method: This is achieved by evaluating current AI agents against the 12th Global Trajectory Optimization Competition (GTOC 12) and utilizing an "LLM-as-a-Judge" methodology to assess performance beyond binary validity.
Results: The evaluation reveals a significant improvement in strategic viability scores over the last two years, but also identifies a critical capability gap between strategy and execution, limiting LLMs to functioning as powerful domain facilitators rather than fully autonomous engineers.
Tags: Judge Prompting Protocols, Metrics And Scoring Methods

### RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes (2602.03462v1)

Authors: Ruwei Pan, Yakun Zhang, Qingyuan Liang, et al.
Published: 2026-02-03
Updated: 2026-02-03
Categories: cs.SE
Link: [arXiv](http://arxiv.org/abs/2602.03462v1)

Abstract: Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .

Summary:
Purpose: The paper proposes RAL-Bench, a benchmark and evaluation framework for assessing the ability of large language models (LLMs) to generate application-level code that meets both functional correctness and non-functional quality attributes.
Method: The RAL-Bench framework distills natural-language requirements from reference projects, builds system tests covering functional and non-functional attributes, and evaluates LLMs based on system-test pass rate and aggregated non-functional quality metrics.
Results: The evaluation of 16 LLMs using RAL-Bench reveals that functional correctness is the dominant bottleneck, with no model exceeding a 45% functional pass rate under the requirement-driven tests.
Tags: Metrics And Scoring Methods, Judge Prompting Protocols

### Precision in Practice: Knowledge Guided Code Summarizing Grounded in Industrial Expectations (2602.03400v1)

Authors: Jintai Li, Songqiang Chen, Shuo Jin, et al.
Published: 2026-02-03
Updated: 2026-02-03
Categories: cs.SE, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.03400v1)

Abstract: Code summaries are essential for helping developers understand code functionality and reducing maintenance and collaboration costs. Although recent advances in large language models (LLMs) have significantly improved automatic code summarization, the practical usefulness of generated summaries in industrial settings remains insufficiently explored. In collaboration with documentation experts from the industrial HarmonyOS project, we conducted a questionnaire study showing that over 57.4% of code summaries produced by state-of-the-art approaches were rejected due to violations of developers' expectations for industrial documentation. Beyond semantic similarity to reference summaries, developers emphasize additional requirements, including the use of appropriate domain terminology, explicit function categorization, and the avoidance of redundant implementation details. To address these expectations, we propose ExpSum, an expectation-aware code summarization approach that integrates function metadata abstraction, informative metadata filtering, context-aware domain knowledge retrieval, and constraint-driven prompting to guide LLMs in generating structured, expectation-aligned summaries. We evaluate ExpSum on the HarmonyOS project and widely used code summarization benchmarks. Experimental results show that ExpSum consistently outperforms all baselines, achieving improvements of up to 26.71% in BLEU-4 and 20.10% in ROUGE-L on HarmonyOS. Furthermore, LLM-based evaluations indicate that ExpSum-generated summaries better align with developer expectations across other projects, demonstrating its effectiveness for industrial code documentation.

Summary:
Purpose: The purpose of this study is to explore the practical usefulness of automatic code summarization in industrial settings and address the insufficiently explored area of code summary quality in relation to developers' expectations.
Method: The method used in this research involves collaborating with documentation experts, conducting a questionnaire study, and proposing an expectation-aware code summarization approach called ExpSum that integrates various techniques to guide large language models in generating structured summaries.
Results: The results of the study show that ExpSum consistently outperforms all baselines, achieving significant improvements in evaluation metrics such as BLEU-4 and ROUGE-L, and demonstrates its effectiveness for industrial code documentation by better aligning with developer expectations.
Tags: Metrics And Scoring Methods, Benchmark And Dataset Creation

### LPS-Bench: Benchmarking Safety Awareness of Computer-Use Agents in Long-Horizon Planning under Benign and Adversarial Scenarios (2602.03255v1)

Authors: Tianyu Chen, Chujia Hu, Ge Gao, et al.
Published: 2026-02-03
Updated: 2026-02-03
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.03255v1)

Abstract: Computer-use agents (CUAs) that interact with real computer systems can perform automated tasks but face critical safety risks. Ambiguous instructions may trigger harmful actions, and adversarial users can manipulate tool execution to achieve malicious goals. Existing benchmarks mostly focus on short-horizon or GUI-based tasks, evaluating on execution-time errors but overlooking the ability to anticipate planning-time risks. To fill this gap, we present LPS-Bench, a benchmark that evaluates the planning-time safety awareness of MCP-based CUAs under long-horizon tasks, covering both benign and adversarial interactions across 65 scenarios of 7 task domains and 9 risk types. We introduce a multi-agent automated pipeline for scalable data generation and adopt an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory. Experiments reveal substantial deficiencies in existing CUAs' ability to maintain safe behavior. We further analyze the risks and propose mitigation strategies to improve long-horizon planning safety in MCP-based CUA systems. We open-source our code at https://github.com/tychenn/LPS-Bench.

Summary:
Purpose: The purpose of this paper is to introduce LPS-Bench, a benchmark that evaluates the planning-time safety awareness of computer-use agents in long-horizon tasks under both benign and adversarial interactions.
Method: Methodologically, the authors employ a multi-agent automated pipeline for scalable data generation and an LLM-as-a-judge evaluation protocol to assess safety awareness through the planning trajectory across various scenarios and task domains.
Results: Results from the experiments reveal substantial deficiencies in existing computer-use agents' ability to maintain safe behavior, highlighting the need for mitigation strategies to improve long-horizon planning safety in these systems.
Tags: Robustness And Sensitivity, Benchmark And Dataset Creation

### MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems (2602.03053v1)

Authors: Vishal Venkataramani, Haizhou Shi, Zixuan Ke, et al.
Published: 2026-02-03
Updated: 2026-02-03
Categories: cs.AI, cs.CL, cs.MA
Link: [arXiv](http://arxiv.org/abs/2602.03053v1)

Abstract: Multi-Agent Systems (MAS) built on Large Language Models (LLMs) often exhibit high variance in their reasoning trajectories. Process verification, which evaluates intermediate steps in trajectories, has shown promise in general reasoning settings, and has been suggested as a potential tool for guiding coordination of MAS; however, its actual effectiveness in MAS remains unclear. To fill this gap, we present MAS-ProVe, a systematic empirical study of process verification for multi-agent systems (MAS). Our study spans three verification paradigms (LLM-as-a-Judge, reward models, and process reward models), evaluated across two levels of verification granularity (agent-level and iteration-level). We further examine five representative verifiers and four context management strategies, and conduct experiments over six diverse MAS frameworks on multiple reasoning benchmarks. We find that process-level verification does not consistently improve performance and frequently exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories. Among the methods studied, LLM-as-a-Judge generally outperforms reward-based approaches, with trained judges surpassing general-purpose LLMs. We further observe a small performance gap between LLMs acting as judges and as single agents, and identify a context-length-performance trade-off in verification. Overall, our results suggest that effective and robust process verification for MAS remains an open challenge, requiring further advances beyond current paradigms. Code is available at https://github.com/Wang-ML-Lab/MAS-ProVe.

Summary:
Purpose: The study aims to investigate the effectiveness of process verification in multi-agent systems (MAS) built on Large Language Models (LLMs) through a systematic empirical evaluation.
Method: The research employs a comprehensive approach, spanning three verification paradigms and two levels of verification granularity, and examines five representative verifiers and four context management strategies across six diverse MAS frameworks.
Results: The findings indicate that process-level verification does not consistently improve performance and often exhibits high variance, highlighting the difficulty of reliably evaluating partial multi-agent trajectories and suggesting that effective process verification for MAS remains an open challenge.
Tags: Benchmark And Dataset Creation
