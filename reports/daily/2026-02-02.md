# LLM as a Judge â€” Daily Report

Report date: 2026-02-02 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 5

## Papers

### Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages (2602.02287v1)

Authors: Isaac Chung, Linda Freienthal
Published: 2026-02-02
Updated: 2026-02-02
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.02287v1)

Abstract: Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences. This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.

Summary:
Purpose: The purpose of this study is to investigate the cross-lingual stability of large language models (LLMs) by evaluating their performance under controlled generation conditions across different languages.
Method: Methodologically, the researchers used synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian to test the stability of automatic metrics and LLM-as-a-judge scoring.
Results: Results showed that while surface-level metrics maintained cross-language stability, pragmatic judgments exhibited ranking instabilities, suggesting that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages.
Tags: Judge Reliability And Calibration, Metrics And Scoring Methods

### Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge (2602.02219v1)

Authors: Yuzheng Xu, Tosho Hirasawa, Tadashi Kozuno, et al.
Published: 2026-02-02
Updated: 2026-02-02
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.02219v1)

Abstract: Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.

Summary:
Purpose: The purpose of this work is to analyze the rubric-based evaluation paradigm in LLM-as-a-judge, which has received less attention compared to point-wise and pair-wise evaluation paradigms.
Method: Methodologically, the study employs controlled experiments across multiple models and datasets to demonstrate consistent position bias in rubric-based evaluation, and proposes a balanced permutation strategy to mitigate this bias.
Results: Results of the study show that aggregating scores across balanced permutations reveals latent position bias and improves correlation between the LLM-as-a-judge and human evaluations, suggesting that simple permutation-based calibration can substantially improve reliability.
Tags: Judge Reliability And Calibration, Judge Prompting Protocols

### Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning (2602.01791v1)

Authors: Zheng Zhang, Ao Lu, Yuanhao Zeng, et al.
Published: 2026-02-02
Updated: 2026-02-02
Categories: cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.01791v1)

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

Summary:
Purpose: The paper introduces Grad2Reward, a novel framework aimed at improving open-ended Large Language Model (LLM) reasoning by addressing the limitations of sparse rewards in Reinforcement Learning with Verifiable Rewards (RLVR).
Method: The Grad2Reward framework extracts dense process rewards directly from the Judge's model inference process via a single backward pass, leveraging gradient-based attribution to enable precise token-level credit assignment.
Results: The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability in enhancing training efficiency and reasoning quality.
Tags: Domain-Specific Judging

### MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark (2602.01714v1)

Authors: Mouath Abu-Daoud, Leen Kharouf, Omar El Hajj, et al.
Published: 2026-02-02
Updated: 2026-02-02
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.01714v1)

Abstract: Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.

Summary:
Purpose: The paper introduces MedAraBench, a large-scale Arabic medical question answering dataset and benchmark, to address the limited availability of open-source data and benchmarks in the Arabic language for natural language processing research.
Method: The dataset was constructed by manually digitizing academic materials created by medical professionals and then preprocessed and split into training and test sets, with its quality assessed through expert human evaluation and LLM-as-a-judge frameworks.
Results: The evaluation of eight state-of-the-art models on the MedAraBench dataset highlights the need for further domain-specific enhancements, and the dataset is released to broaden the diversity of medical data benchmarks and enhance the multilingual capabilities of Large Language Models.
Tags: Benchmark And Dataset Creation, Domain-Specific Judging

### Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning (2602.01528v1)

Authors: Qian Wang, Xuandong Zhao, Zirui Zhang, et al.
Published: 2026-02-02
Updated: 2026-02-02
Categories: cs.CY, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.01528v1)

Abstract: Large language models (LLMs) increasingly serve as automated judges, yet they remain susceptible to cognitive biases -- often altering their reasoning when faced with spurious prompt-level cues such as consensus claims or authority appeals. Existing mitigations via prompting or supervised fine-tuning fail to generalize, as they modify surface behavior without changing the optimization objective that makes bias cues predictive. To address this gap, we propose Epistemic Independence Training (EIT), a reinforcement learning framework grounded in a key principle: to learn independence, bias cues must be made non-predictive of reward. EIT operationalizes this through a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers, combined with a reward design that penalizes bias-following without rewarding bias agreement. Experiments on Qwen3-4B demonstrate that EIT improves both accuracy and robustness under adversarial biases, while preserving performance when bias aligns with truth. Notably, models trained only on bandwagon bias generalize to unseen bias types such as authority and distraction, indicating that EIT induces transferable epistemic independence rather than bias-specific heuristics. Code and data are available at https://anonymous.4open.science/r/bias-mitigation-with-rl-BC47.

Summary:
Purpose: The purpose of this paper is to propose a reinforcement learning framework called Epistemic Independence Training (EIT) to train robust large language model judges that are resistant to cognitive biases.
Method: Method: The method used in this paper involves a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers, combined with a reward design that penalizes bias-following without rewarding bias agreement.
Results: Results: The results of the experiments demonstrate that EIT improves both accuracy and robustness under adversarial biases, while preserving performance when bias aligns with truth, and induces transferable epistemic independence rather than bias-specific heuristics.
Tags: Robustness And Sensitivity, Judge Reliability And Calibration
