# LLM as a Judge â€” Daily Report

Report date: 2026-02-04 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 6

## Papers

### Rethinking Rubric Generation for Improving LLM Judge and Reward Modeling for Open-ended Tasks (2602.05125v1)

Authors: William F. Shen, Xinchi Qiu, Chenxi Whitehouse, et al.
Published: 2026-02-04
Updated: 2026-02-04
Categories: cs.LG, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.05125v1)

Abstract: Recently, rubrics have been used to guide LLM judges in capturing subjective, nuanced, multi-dimensional human preferences, and have been extended from evaluation to reward signals for reinforcement fine-tuning (RFT). However, rubric generation remains hard to control: rubrics often lack coverage, conflate dimensions, misalign preference direction, and contain redundant or highly correlated criteria, degrading judge accuracy and producing suboptimal rewards during RFT. We propose RRD, a principled framework for rubric refinement built on a recursive decompose-filter cycle. RRD decomposes coarse rubrics into fine-grained, discriminative criteria, expanding coverage while sharpening separation between responses. A complementary filtering mechanism removes misaligned and redundant rubrics, and a correlation-aware weighting scheme prevents over-representing highly correlated criteria, yielding rubric sets that are informative, comprehensive, and non-redundant. Empirically, RRD delivers large, consistent gains across both evaluation and training: it improves preference-judgment accuracy on JudgeBench and PPE for both GPT-4o and Llama3.1-405B judges, achieving top performance in all settings with up to +17.7 points on JudgeBench. When used as the reward source for RFT on WildChat, it yields substantially stronger and more stable learning signals, boosting reward by up to 160% (Qwen3-4B) and 60% (Llama3.1-8B) versus 10-20% for prior rubric baselines, with gains that transfer to HealthBench-Hard and BiGGen Bench. Overall, RRD establishes recursive rubric refinement as a scalable and interpretable foundation for LLM judging and reward modeling in open-ended domains.

Summary:
Purpose: The paper aims to improve the generation of rubrics for guiding large language model (LLM) judges and reward modeling in open-ended tasks by addressing the limitations of existing rubric generation methods.
Method: The authors propose a principled framework called RRD, which uses a recursive decompose-filter cycle to refine coarse rubrics into fine-grained, discriminative criteria and remove misaligned and redundant rubrics.
Results: The proposed RRD framework delivers significant gains in preference-judgment accuracy and reward signals, outperforming prior baselines and achieving top performance in various settings with substantial improvements in judge accuracy and learning stability.
Tags: Judge Prompting Protocols, Judge Reliability And Calibration

### Understanding LLM Evaluator Behavior: A Structured Multi-Evaluator Framework for Merchant Risk Assessment (2602.05110v1)

Authors: Liang Wang, Junpeng Wang, Chin-chia Michael Yeh, et al.
Published: 2026-02-04
Updated: 2026-02-04
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.05110v1)

Abstract: Large Language Models (LLMs) are increasingly used as evaluators of reasoning quality, yet their reliability and bias in payments-risk settings remain poorly understood. We introduce a structured multi-evaluator framework for assessing LLM reasoning in Merchant Category Code (MCC)-based merchant risk assessment, combining a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability. Five frontier LLMs generate and cross-evaluate MCC risk rationales under attributed and anonymized conditions. To establish a judge-independent reference, we introduce a consensus-deviation metric that eliminates circularity by comparing each judge's score to the mean of all other judges, yielding a theoretically grounded measure of self-evaluation and cross-model deviation. Results reveal substantial heterogeneity: GPT-5.1 and Claude 4.5 Sonnet show negative self-evaluation bias (-0.33, -0.31), while Gemini-2.5 Pro and Grok 4 display positive bias (+0.77, +0.71), with bias attenuating by 25.8 percent under anonymization. Evaluation by 26 payment-industry experts shows LLM judges assign scores averaging +0.46 points above human consensus, and that the negative bias of GPT-5.1 and Claude 4.5 Sonnet reflects closer alignment with human judgment. Ground-truth validation using payment-network data shows four models exhibit statistically significant alignment (Spearman rho = 0.56 to 0.77), confirming that the framework captures genuine quality. Overall, the framework provides a replicable basis for evaluating LLM-as-a-judge systems in payment-risk workflows and highlights the need for bias-aware protocols in operational financial settings.

Summary:
Purpose: The paper aims to understand the behavior of Large Language Models (LLMs) as evaluators of reasoning quality in payments-risk settings, particularly in Merchant Category Code (MCC)-based merchant risk assessment.
Method: The study introduces a structured multi-evaluator framework that combines a five-criterion rubric with Monte-Carlo scoring to evaluate rationale quality and evaluator stability, and also introduces a consensus-deviation metric to establish a judge-independent reference.
Results: The results show substantial heterogeneity in LLM evaluation behavior, with some models exhibiting negative self-evaluation bias and others showing positive bias, and the framework is validated using payment-network data, confirming that it captures genuine quality.
Tags: Judge Reliability And Calibration, Metrics And Scoring Methods

### VERA-MH: Reliability and Validity of an Open-Source AI Safety Evaluation in Mental Health (2602.05088v3)

Authors: Kate H. Bentley, Luca Belli, Adam M. Chekroud, et al.
Published: 2026-02-04
Updated: 2026-02-17
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.05088v3)

Abstract: Millions now use generative AI chatbots for psychological support. Despite the promise related to availability and scale, the single most pressing question in AI for mental health is whether these tools are safe. The Validation of Ethical and Responsible AI in Mental Health (VERA-MH) evaluation was recently proposed to meet the urgent need for an evidence-based, automated safety benchmark. This study aimed to examine the clinical validity and reliability of VERA-MH for evaluating AI safety in suicide risk detection and response. We first simulated a large set of conversations between large language model (LLM)-based users (user-agents) and general-purpose AI chatbots. Licensed mental health clinicians used a rubric (scoring guide) to independently rate the simulated conversations for safe and unsafe chatbot behaviors, as well as user-agent realism. An LLM-based judge used the same scoring rubric to evaluate the same set of simulated conversations. We then examined rating alignment (a) among individual clinicians and (b) between clinician consensus and the LLM judge, and (c) summarized clinicians' ratings of user-agent realism. Individual clinicians were generally consistent with one another in their safety ratings (chance-corrected inter-rater reliability [IRR] = 0.77), establishing a gold-standard clinical reference. The LLM judge was strongly aligned with this clinical consensus overall (IRR = 0.81) and within key conditions. Together, findings from this human evaluation study support the validity and reliability of VERA-MH: an open-source, automated AI safety evaluation for mental health. Future research will examine the generalizability and robustness of VERA-MH and expand the framework to target additional key areas of AI safety in mental health.

Summary:
Purpose: The study aimed to examine the clinical validity and reliability of VERA-MH, an open-source AI safety evaluation, for assessing the safety of AI chatbots in detecting and responding to suicide risk.
Method: The researchers simulated conversations between AI chatbots and users, which were then rated by licensed mental health clinicians and an LLM-based judge using a scoring rubric to evaluate safe and unsafe chatbot behaviors.
Results: The study found that VERA-MH has strong validity and reliability, with high inter-rater reliability among clinicians and alignment between clinician consensus and the LLM judge, supporting its use as an automated AI safety evaluation for mental health.
Tags: Judge Reliability And Calibration, Judge Prompting Protocols

### Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models (2602.04649v1)

Authors: Binghai Wang, Yantao Liu, Yuxuan Liu, et al.
Published: 2026-02-04
Updated: 2026-02-04
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.04649v1)

Abstract: Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.

Summary:
Purpose: The purpose of this paper is to address the issue of deceptive alignment in Generative Reward Models (GenRMs) and LLM-as-a-Judge, which prioritize Outcome Accuracy over correct reasoning processes.
Method: The method used to tackle this problem involves introducing Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment, and combining it with outcome accuracy for GenRM training.
Results: The results show that the proposed hybrid signal achieves state-of-the-art performance on several benchmarks, effectively detects deceptive alignment, and improves performance in RLHF tasks, while also reversing the decline in rationale consistency observed in outcome-only training.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### PersoDPO: Scalable Preference Optimization for Instruction-Adherent, Persona-Grounded Dialogue via Multi-LLM Evaluation (2602.04493v1)

Authors: Saleh Afzoon, MohammadHossein Ahmadi, Usman Naseem, et al.
Published: 2026-02-04
Updated: 2026-02-04
Categories: cs.CL, cs.HC
Link: [arXiv](http://arxiv.org/abs/2602.04493v1)

Abstract: Personalization and contextual coherence are two essential components in building effective persona-grounded dialogue systems. These aspects play a crucial role in enhancing user engagement and ensuring responses are more relevant and consistent with user identity. However, recent studies indicate that open-source large language models (LLMs) continue to struggle to generate responses that are both contextually grounded and aligned with persona cues, despite exhibiting strong general conversational abilities like fluency and naturalness. We present PersoDPO, a scalable preference optimisation framework that uses supervision signals from automatic evaluations of responses generated by both closed-source and open-source LLMs to fine-tune dialogue models. The framework integrates evaluation metrics targeting coherence and personalization, along with a length-format compliance feature to promote instruction adherence. These signals are combined to automatically construct high-quality preference pairs without manual annotation, enabling a scalable and reproducible training pipeline. Experiments on the FoCus dataset show that an open-source language model fine-tuned with the PersoDPO framework consistently outperforms strong open-source baselines and a standard Direct Preference Optimization (DPO) variant across multiple evaluation dimensions.

Summary:
Purpose: The paper aims to enhance persona-grounded dialogue systems by developing a scalable preference optimization framework that promotes personalization and contextual coherence in generated responses.
Method: The PersoDPO framework uses automatic evaluations of responses from large language models to fine-tune dialogue models, integrating metrics for coherence, personalization, and instruction adherence without requiring manual annotation.
Results: Experiments demonstrate that an open-source language model fine-tuned with the PersoDPO framework outperforms strong baselines and a standard Direct Preference Optimization variant across multiple evaluation dimensions on the FoCus dataset.
Tags: Metrics And Scoring Methods, Benchmark And Dataset Creation

### The Supportiveness-Safety Tradeoff in LLM Well-Being Agents (2602.04487v1)

Authors: Himanshi Lalwani, Hanan Salam
Published: 2026-02-04
Updated: 2026-02-04
Categories: cs.HC, cs.RO
Link: [arXiv](http://arxiv.org/abs/2602.04487v1)

Abstract: Large language models (LLMs) are being integrated into socially assistive robots (SARs) and other conversational agents providing mental health and well-being support. These agents are often designed to sound empathic and supportive in order to maximize user's engagement, yet it remains unclear how increasing the level of supportive framing in system prompts influences safety relevant behavior. We evaluated 6 LLMs across 3 system prompts with varying levels of supportiveness on 80 synthetic queries spanning 4 well-being domains (1440 responses). An LLM judge framework, validated against human ratings, assessed safety and care quality. Moderately supportive prompts improved empathy and constructive support while maintaining safety. In contrast, strongly validating prompts significantly degraded safety and, in some cases, care across all domains, with substantial variation across models. We discuss implications for prompt design, model selection, and domain specific safeguards in SARs deployment.

Summary:
Purpose: The paper investigates the tradeoff between supportiveness and safety in large language models (LLMs) used in socially assistive robots and conversational agents for mental health and well-being support.
Method: The study evaluated 6 LLMs across 3 system prompts with varying levels of supportiveness on 80 synthetic queries, using an LLM judge framework validated against human ratings to assess safety and care quality.
Results: The results showed that moderately supportive prompts improved empathy and constructive support while maintaining safety, whereas strongly validating prompts degraded safety and care quality across all domains.
Tags: Judge Reliability And Calibration
