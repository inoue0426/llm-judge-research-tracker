# LLM as a Judge â€” Daily Report

Report date: 2026-02-09 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 7

## Papers

### How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs (2602.08808v1)

Authors: Yapei Chang, Kyle Lo, Mohit Iyyer, et al.
Published: 2026-02-09
Updated: 2026-02-09
Categories: cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.08808v1)

Abstract: Generating step-by-step "how-to" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.

Summary:
Purpose: The paper aims to address the challenge of measuring and improving procedural validity at scale on real-world tasks for large language models (LLMs) by introducing a scalable framework called How2Everything.
Method: The framework includes mining procedures from web pages, building an evaluation set, and developing an evaluation protocol that uses an LLM judge to score model outputs, as well as distilling a frontier model into a smaller open model.
Results: The How2Everything framework achieves significant improvements in performance on the evaluation set, with gains of over 10 points across three models, and provides a closed loop of capability evaluation and improvement at scale using pretraining web data.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### Taming Scylla: Understanding the multi-headed agentic daemon of the coding seas (2602.08765v1)

Authors: Micah Villmow
Published: 2026-02-09
Updated: 2026-02-09
Categories: cs.SE, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.08765v1)

Abstract: LLM-based tools are automating more software development tasks at a rapid pace, but there is no rigorous way to evaluate how different architectural choices -- prompts, skills, tools, multi-agent setups -- materially affect both capability and cost. This paper introduces Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies that uses seven testing tiers (T0-T6) progressively adding complexity to isolate what directly influences results and how. The key metric is Cost-of-Pass (CoP): the expected dollar cost to get one correct solution, which directly quantifies the trade-off between complexity and efficiency. The framework is model-agnostic, designed to work with any CLI tool; this paper demonstrates it with Claude Sonnet 4.5, using multiple LLM judges (Opus 4.5, Sonnet 4.5, Haiku 4.5) from the same vendor for evaluation consensus, where judges score results using direct tests, human-designed LLM-evaluated rubrics, and qualitative assessment. The result is a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes, suggesting that architectural complexity does not always improve quality.

Summary:
Purpose: The paper aims to introduce Scylla, an evaluation framework for benchmarking agentic coding tools through structured ablation studies to understand the impact of different architectural choices on capability and cost.
Method: The framework uses seven testing tiers progressively adding complexity to isolate what directly influences results and how, with a key metric being Cost-of-Pass, which quantifies the trade-off between complexity and efficiency.
Results: The study demonstrates that architectural complexity does not always improve quality, providing a reproducible framework that quantifies trade-offs between agent complexity and actual outcomes using multiple LLM judges for evaluation consensus.
Tags: Metrics And Scoring Methods, Judge Prompting Protocols

### Learning to Judge: LLMs Designing and Applying Evaluation Rubrics (2602.08672v1)

Authors: Clemencia Siro, Pourya Aliannejadi, Mohammad Aliannejadi
Published: 2026-02-09
Updated: 2026-02-09
Categories: cs.CL, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.08672v1)

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

Summary:
Purpose: The purpose of this study is to investigate whether large language models (LLMs) can design and apply their own evaluation rubrics, rather than relying on human-defined ones.
Method: The method used in this study involves introducing GER-Eval, a framework that enables LLMs to generate and apply their own evaluation criteria, which are then evaluated for semantic coherence, scoring reliability, and alignment with human criteria.
Results: The results show that LLMs can reliably generate interpretable evaluation dimensions, but their scoring reliability degrades in factual settings, and closed-source models achieve higher agreement and cross-model generalization than open-weight models.
Tags: Judge Reliability And Calibration, Judge Prompting Protocols

### ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems (2602.08567v1)

Authors: Jinnuo Liu, Chuke Liu, Hua Shen
Published: 2026-02-09
Updated: 2026-02-09
Categories: cs.MA, cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.08567v1)

Abstract: Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.

Summary:
Purpose: The paper aims to introduce ValueFlow, a framework for measuring and analyzing the propagation of value perturbations in multi-agent large language model systems.
Method: The authors present a perturbation-based evaluation approach using a 56-value dataset derived from the Schwartz Value Survey and an LLM-as-a-judge protocol to quantify agents' value orientations during interaction.
Results: The experiments demonstrate that susceptibility to value perturbations varies widely across values and is strongly shaped by structural topology, as measured by metrics such as beta-susceptibility and system susceptibility.
Tags: Robustness And Sensitivity, Benchmark And Dataset Creation

### Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI (2602.08268v2)

Authors: Akinori Maeda, Yuto Sekiya, Sota Sugimura, et al.
Published: 2026-02-09
Updated: 2026-02-10
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.08268v2)

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

Summary:
Purpose: The paper proposes a solution to balance the utilization of personal data with privacy protection in the context of personalized AI services, addressing the challenge of restricted user sovereignty due to siloed ecosystems.
Method: To achieve this, the authors designed and implemented Puda, a user-sovereign architecture that aggregates data across services and enables client-side management, allowing users to control data sharing at three privacy levels.
Results: The evaluation of Puda through a personalized travel planning task shows that it can achieve effective multi-granularity management, with one of the privacy settings achieving 97.2% of the personalization performance obtained when sharing more detailed data.
Tags: Judge Prompting Protocols, Benchmark And Dataset Creation

### A Statistical Framework for Alignment with Biased AI Feedback (2602.08259v1)

Authors: Xintao Xia, Zhiqiu Xia, Linjun Zhang, et al.
Published: 2026-02-09
Updated: 2026-02-09
Categories: stat.ML, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.08259v1)

Abstract: Modern alignment pipelines are increasingly replacing expensive human preference labels with evaluations from large language models (LLM-as-Judge). However, AI labels can be systematically biased compared to high-quality human feedback datasets. In this paper, we develop two debiased alignment methods within a general framework that accommodates heterogeneous prompt-response distributions and external human feedback sources. Debiased Direct Preference Optimization (DDPO) augments standard DPO with a residual-based correction and density-ratio reweighting to mitigate systematic bias, while retaining DPO's computational efficiency. Debiased Identity Preference Optimization (DIPO) directly estimates human preference probabilities without imposing a parametric reward model. We provide theoretical guarantees for both methods: DDPO offers a practical and computationally efficient solution for large-scale alignment, whereas DIPO serves as a robust, statistically optimal alternative that attains the semiparametric efficiency bound. Empirical studies on sentiment generation, summarization, and single-turn dialogue demonstrate that the proposed methods substantially improve alignment efficiency and recover performance close to that of an oracle trained on fully human-labeled data.

Summary:
Purpose: The paper aims to develop a statistical framework for alignment with biased AI feedback, addressing the issue of systematic bias in AI labels compared to high-quality human feedback datasets.
Method: The authors propose two debiased alignment methods, Debiased Direct Preference Optimization (DDPO) and Debiased Identity Preference Optimization (DIPO), which accommodate heterogeneous prompt-response distributions and external human feedback sources.
Results: The proposed methods demonstrate substantial improvement in alignment efficiency and recover performance close to that of an oracle trained on fully human-labeled data, as shown in empirical studies on sentiment generation, summarization, and single-turn dialogue.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation (2602.08229v1)

Authors: Yifan Yang, Jinjia Li, Kunxi Li, et al.
Published: 2026-02-09
Updated: 2026-02-09
Categories: cs.AI, cs.CR, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.08229v1)

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

Summary:
Purpose: The purpose of this paper is to propose a decentralized evaluation framework for large language models (LLMs) to address the limitations of current centralized evaluation methods, which suffer from opacity, overfitting, and hardware-induced variance.
Method: Methodologically, the authors leverage a blockchain-based protocol to enable hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes, incentivizing global contributors to act as independent validators using a robust reward system.
Results: Results of the empirical analysis demonstrate that the proposed decentralized evaluation framework significantly reduces the standard deviation across ten runs on the same model to 0.28, ensuring higher statistical confidence in model rankings compared to conventional frameworks.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation
