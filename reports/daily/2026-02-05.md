# LLM as a Judge — Daily Report

Report date: 2026-02-05 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 7

## Papers

### BenchMarker: An Education-Inspired Toolkit for Highlighting Flaws in Multiple-Choice Benchmarks (2602.06221v1)

Authors: Nishant Balepur, Bhavya Rajasekaran, Jane Oh, et al.
Published: 2026-02-05
Updated: 2026-02-05
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.06221v1)

Abstract: Multiple-choice question answering (MCQA) is standard in NLP, but benchmarks lack rigorous quality control. We present BenchMarker, an education-inspired toolkit using LLM judges to flag three common MCQ flaws: 1) contamination - items appearing exactly online; 2) shortcuts - cues in the choices that enable guessing; and 3) writing errors - structural/grammatical issues based on a 19-rule education rubric. We validate BenchMarker with human annotations, then run the tool to audit 12 benchmarks, revealing: 2) contaminated MCQs tend to inflate accuracy, while writing errors tend to lower it and change rankings beyond random; and 3) prior benchmark repairs address their targeted issues (i.e., lowering accuracy with LLM-written distractors), but inadvertently add new flaws (i.e. implausible distractors, many correct answers). Overall, flaws in MCQs degrade NLP evaluation, but education research offers a path forward. We release BenchMarker to bridge the fields and improve MCQA benchmark design.

Summary:
Purpose: The purpose of the paper is to present BenchMarker, an education-inspired toolkit designed to identify and address common flaws in multiple-choice benchmarks used in natural language processing (NLP).
Method: Method: The toolkit utilizes large language model (LLM) judges to flag three types of flaws, including contamination, shortcuts, and writing errors, based on a 19-rule education rubric, and is validated with human annotations.
Results: Results: The application of BenchMarker to audit 12 benchmarks reveals that contaminated MCQs tend to inflate accuracy, while writing errors lower it and change rankings, highlighting the need for improved benchmark design to ensure accurate NLP evaluation.
Tags: Judge Prompting Protocols, Benchmark And Dataset Creation

### Reinforcement World Model Learning for LLM-based Agents (2602.05842v2)

Authors: Xiao Yu, Baolin Peng, Ruize Xu, et al.
Published: 2026-02-05
Updated: 2026-02-09
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.05842v2)

Abstract: Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $τ^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $τ^2$ Bench respectively, while matching the performance of expert-data training.

Summary:
Purpose: The purpose of this paper is to propose a self-supervised method called Reinforcement World Model Learning (RWML) that enables large language models (LLMs) to learn action-conditioned world models for better performance in agentic settings.
Method: Methodologically, RWML learns to align simulated next states with realized next states from the environment using sim-to-real gap rewards, providing a more robust training signal than traditional next-state token prediction methods.
Results: Results show that RWML achieves significant gains over the base model on ALFWorld and $τ^2$ Bench benchmarks, outperforming direct task-success reward reinforcement learning and matching expert-data training performance when combined with task-success rewards.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### Generative Ontology: When Structured Knowledge Learns to Create (2602.05636v2)

Authors: Benny Cheung
Published: 2026-02-05
Updated: 2026-02-09
Categories: cs.AI, cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.05636v2)

Abstract: Traditional ontologies describe domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs lacking structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework synthesizing these complementary strengths: ontology provides the grammar; the LLM provides the creativity. Generative Ontology encodes domain knowledge as executable Pydantic schemas constraining LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits, each carrying a professional "anxiety" that prevents shallow outputs. Retrieval-augmented generation grounds designs in precedents from existing exemplars. We demonstrate the framework through GameGrammar, generating complete tabletop game designs, and present three empirical studies. An ablation study (120 designs, 4 conditions) shows multi-agent specialization produces the largest quality gains (fun d=1.12, depth d=1.59; p<.001), while schema validation eliminates structural errors (d=4.78). A benchmark against 20 published board games reveals structural parity but a bounded creative gap (fun d=1.86): generated designs score 7-8 while published games score 8-9. A test-retest study (50 evaluations) validates the LLM-based evaluator, with 7/9 metrics achieving Good-to-Excellent reliability (ICC 0.836-0.989). The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars is a candidate for Generative Ontology.

Summary:
Purpose: The paper introduces Generative Ontology, a framework that combines the strengths of traditional ontologies and large language models to generate novel artifacts with structural validity.
Method: The framework encodes domain knowledge as executable Pydantic schemas and uses a multi-agent pipeline with specialized roles to constrain LLM generation and produce high-quality outputs.
Results: The empirical studies demonstrate the effectiveness of Generative Ontology, showing significant quality gains, elimination of structural errors, and reliable evaluation metrics, with potential applications generalizing beyond games to various domains.
Tags: Metrics And Scoring Methods, Benchmark And Dataset Creation

### Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations (2602.05523v1)

Authors: Shahin Honarvar, Amber Gorzynski, James Lee-Jones, et al.
Published: 2026-02-05
Updated: 2026-02-05
Categories: cs.SE, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.05523v1)

Abstract: Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.

Summary:
Purpose: The paper introduces a new evaluation method for agentic large language models (LLMs) using capture-the-flag (CTF) challenge families to assess their robustness and generalisation abilities across alternative versions of source code.
Method: The authors use a tool called Evolve-CTF to generate CTF families from Python challenges via semantics-preserving program transformations, allowing for controlled evaluation of agent robustness to source code transformations.
Results: The evaluation of 13 agentic LLM configurations reveals that models are robust to certain transformations but struggle with composed transformations and deeper obfuscation, and that enabling explicit reasoning has little effect on solution success rates across challenge families.
Tags: Benchmark And Dataset Creation, Robustness And Sensitivity

### BadTemplate: A Training-Free Backdoor Attack via Chat Template Against Large Language Models (2602.05401v1)

Authors: Zihan Wang, Hongwei Li, Rui Zhang, et al.
Published: 2026-02-05
Updated: 2026-02-05
Categories: cs.CR
Link: [arXiv](http://arxiv.org/abs/2602.05401v1)

Abstract: Chat template is a common technique used in the training and inference stages of Large Language Models (LLMs). It can transform input and output data into role-based and templated expressions to enhance the performance of LLMs. However, this also creates a breeding ground for novel attack surfaces. In this paper, we first reveal that the customizability of chat templates allows an attacker who controls the template to inject arbitrary strings into the system prompt without the user's notice. Building on this, we propose a training-free backdoor attack, termed BadTemplate. Specifically, BadTemplate inserts carefully crafted malicious instructions into the high-priority system prompt, thereby causing the target LLM to exhibit persistent backdoor behaviors. BadTemplate outperforms traditional backdoor attacks by embedding malicious instructions directly into the system prompt, eliminating the need for model retraining while achieving high attack effectiveness with minimal cost. Furthermore, its simplicity and scalability make it easily and widely deployed in real-world systems, raising serious risks of rapid propagation, economic damage, and large-scale misinformation. Furthermore, detection by major third-party platforms HuggingFace and LLM-as-a-judge proves largely ineffective against BadTemplate. Extensive experiments conducted on 5 benchmark datasets across 6 open-source and 3 closed-source LLMs, compared with 3 baselines, demonstrate that BadTemplate achieves up to a 100% attack success rate and significantly outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks. Our work highlights the potential security risks raised by chat templates in the LLM supply chain, thereby supporting the development of effective defense mechanisms.

Summary:
Purpose: The paper aims to reveal the vulnerability of Large Language Models (LLMs) to a novel training-free backdoor attack via chat template, which can inject arbitrary strings into the system prompt without user notice.
Method: The proposed BadTemplate attack inserts carefully crafted malicious instructions into the high-priority system prompt, causing the target LLM to exhibit persistent backdoor behaviors without requiring model retraining.
Results: Extensive experiments demonstrate that BadTemplate achieves up to a 100% attack success rate and outperforms traditional prompt-based backdoors in both word-level and sentence-level attacks across various benchmark datasets and LLMs.
Tags: Benchmark And Dataset Creation

### Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances (2602.05392v1)

Authors: Jiyun Chun, Eric Fosler-Lussier, Michael White, et al.
Published: 2026-02-05
Updated: 2026-02-05
Categories: cs.CL, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.05392v1)

Abstract: Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.

Summary:
Purpose: The purpose of this paper is to introduce a new framework for evaluating the quality of children's utterances in adult-child dialogue, moving beyond traditional metrics that focus on length and ignoring conversational context.
Method: The method used in this study involves an LLM-as-a-judge framework that classifies the Previous Adult Utterance Type and scores the child's response along two axes: Expansion and Independence, which reflect fundamental dimensions in child language development.
Results: The results of this study demonstrate the developmental validity and predictive value of the proposed metrics, which align with human judgments and enable large-scale evaluation of child utterances, shifting the focus from measuring length to evaluating meaningful contributions to the conversation.
Tags: Metrics And Scoring Methods, Robustness And Sensitivity

### Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation (2602.05381v1)

Authors: Ting Fang Tan, Kabilan Elangovan, Andreas Pollreisz, et al.
Published: 2026-02-05
Updated: 2026-02-05
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.05381v1)

Abstract: Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.

Summary:
Purpose: The study aims to evaluate the performance of four small medical large language models in answering ophthalmology-related patient queries and assess the feasibility of using another large language model for evaluation against clinician grading.
Method: The researchers used a cross-sectional study design, where 180 ophthalmology patient queries were answered by each of the four models, and the responses were evaluated by three ophthalmologists and a large language model called GPT-4-Turbo using the S.C.O.R.E. framework.
Results: The results showed that one of the models, Meerkat-7B, achieved the highest performance, while another model, MedLLaMA3-v20, performed poorly, and the evaluation by GPT-4-Turbo showed strong alignment with clinician assessments, supporting the feasibility of large language model-based evaluation for clinical deployment.
Tags: Domain-Specific Judging, Benchmark And Dataset Creation
