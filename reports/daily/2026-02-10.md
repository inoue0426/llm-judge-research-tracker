# LLM as a Judge — Daily Report

Report date: 2026-02-10 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 8

## Papers

### Simple LLM Baselines are Competitive for Model Diffing (2602.10371v1)

Authors: Elias Kempf, Simon Schrodi, Bartosz Cywiński, et al.
Published: 2026-02-10
Updated: 2026-02-10
Categories: cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.10371v1)

Abstract: Standard LLM evaluations only test capabilities or dispositions that evaluators designed them for, missing unexpected differences such as behavioral shifts between model revisions or emergent misaligned tendencies. Model diffing addresses this limitation by automatically surfacing systematic behavioral differences. Recent approaches include LLM-based methods that generate natural language descriptions and sparse autoencoder (SAE)-based methods that identify interpretable features. However, no systematic comparison of these approaches exists nor are there established evaluation criteria. We address this gap by proposing evaluation metrics for key desiderata (generalization, interestingness, and abstraction level) and use these to compare existing methods. Our results show that an improved LLM-based baseline performs comparably to the SAE-based method while typically surfacing more abstract behavioral differences.

Summary:
Purpose: The purpose of this paper is to address the limitation of standard LLM evaluations by proposing a systematic comparison of model diffing approaches to surface unexpected differences between model revisions.
Method: Method: The authors propose evaluation metrics for key desiderata such as generalization, interestingness, and abstraction level, and use these to compare existing LLM-based and SAE-based methods.
Results: Results: The results show that an improved LLM-based baseline performs comparably to the SAE-based method, while typically surfacing more abstract behavioral differences between model revisions.
Tags: Metrics And Scoring Methods, Judge Prompting Protocols

### LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation (2602.10367v1)

Authors: Zhiling Yan, Dingjie Song, Zhe Fang, et al.
Published: 2026-02-10
Updated: 2026-02-10
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.10367v1)

Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current evaluation metrics for open-ended clinical reasoning often rely on either shallow lexical overlap (e.g., ROUGE) or subjective LLM-as-a-Judge scoring, both inadequate for verifying clinical correctness. To bridge these gaps, we introduce LiveMedBench, a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities, ensuring strict temporal separation from model training data. We propose a Multi-Agent Clinical Curation Framework that filters raw data noise and validates clinical integrity against evidence-based medical principles. For evaluation, we develop an Automated Rubric-based Evaluation Framework that decomposes physician responses into granular, case-specific criteria, achieving substantially stronger alignment with expert physicians than LLM-as-a-Judge. To date, LiveMedBench comprises 2,756 real-world cases spanning 38 medical specialties and multiple languages, paired with 16,702 unique evaluation criteria. Extensive evaluation of 38 LLMs reveals that even the best-performing model achieves only 39.2%, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks. Error analysis further identifies contextual application-not factual knowledge-as the dominant bottleneck, with 35-48% of failures stemming from the inability to tailor medical knowledge to patient-specific constraints.

Summary:
Purpose: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation, which is the primary goal of the LiveMedBench benchmark.
Method: To bridge the gaps in existing medical benchmarks, the authors introduce a continuously updated, contamination-free, and rubric-based benchmark that weekly harvests real-world clinical cases from online medical communities using a Multi-Agent Clinical Curation Framework.
Results: The extensive evaluation of 38 LLMs on LiveMedBench reveals that even the best-performing model achieves only 39.2% accuracy, and 84% of models exhibit performance degradation on post-cutoff cases, confirming pervasive data contamination risks.
Tags: Metrics And Scoring Methods, Judge Prompting Protocols

### SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation (2602.10017v1)

Authors: Homaira Huda Shomee, Rochana Chaturvedi, Yangxinyu Xie, et al.
Published: 2026-02-10
Updated: 2026-02-10
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.10017v1)

Abstract: Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question-answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.

Summary:
Purpose: The paper proposes a new evaluation framework to assess the effectiveness of large language models in providing fine-grained, decision-critical details in domain-specific settings.
Method: The proposed framework evaluates model outputs along four dimensions - specificity, robustness, answer relevance, and context utilization - using a curated dataset of 1,412 question-answer pairs and human evaluation to assess inter-annotator agreement.
Results: The study finds that no single metric is sufficient to capture answer quality in isolation, highlighting the need for structured, multi-metric evaluation frameworks when deploying language models in high-stakes applications.
Tags: Judge Reliability And Calibration, Robustness And Sensitivity

### Closing Reasoning Gaps in Clinical Agents with Differential Reasoning Learning (2602.09945v1)

Authors: Jinsong Liu, Yuhang Jiang, Ramayya Krishnan, et al.
Published: 2026-02-10
Updated: 2026-02-10
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.09945v1)

Abstract: Clinical decision support requires not only correct answers but also clinically valid reasoning. We propose Differential Reasoning Learning (DRL), a framework that improves clinical agents by learning from reasoning discrepancies. From reference reasoning rationales (e.g., physician-authored clinical rationale, clinical guidelines, or outputs from more capable models) and the agent's free-form chain-of-thought (CoT), DRL extracts reasoning graphs as directed acyclic graphs (DAGs) and performs a clinically weighted graph edit distance (GED)-based discrepancy analysis. An LLM-as-a-judge aligns semantically equivalent nodes and diagnoses discrepancies between graphs. These graph-level discrepancy diagnostics are converted into natural-language instructions and stored in a Differential Reasoning Knowledge Base (DR-KB). At inference, we retrieve top-$k$ instructions via Retrieval-Augmented Generation (RAG) to augment the agent prompt and patch likely logic gaps. Evaluation on open medical question answering (QA) benchmarks and a Return Visit Admissions (RVA) prediction task from internal clinical data demonstrates gains over baselines, improving both final-answer accuracy and reasoning fidelity. Ablation studies confirm gains from infusing reference reasoning rationales and the top-$k$ retrieval strategy. Clinicians' review of the output provides further assurance of the approach. Together, results suggest that DRL supports more reliable clinical decision-making in complex reasoning scenarios and offers a practical mechanism for deployment under limited token budgets.

Summary:
Purpose: The paper proposes Differential Reasoning Learning (DRL), a framework designed to improve clinical agents by learning from reasoning discrepancies and enhancing their ability to provide clinically valid reasoning.
Method: The DRL framework extracts reasoning graphs, performs discrepancy analysis using graph edit distance, and utilizes a large language model to align semantically equivalent nodes and diagnose discrepancies between graphs.
Results: Evaluation of the DRL framework on medical question answering benchmarks and a clinical prediction task demonstrates significant gains over baselines in terms of final-answer accuracy and reasoning fidelity, with ablation studies confirming the effectiveness of the approach.
Tags: Domain-Specific Judging, Judge Prompting Protocols

### Decomposing Reasoning Efficiency in Large Language Models (2602.09805v1)

Authors: Daniel Kaiser, Arnoldo Frigessi, Ali Ramezani-Kebrya, et al.
Published: 2026-02-10
Updated: 2026-02-10
Categories: cs.CL, cs.AI, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.09805v1)

Abstract: Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $ρ=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.

Summary:
Purpose: The purpose of this paper is to introduce a framework for decomposing reasoning efficiency in large language models into interpretable factors to better understand where tokens are spent or wasted during inference.
Method: Method: The authors propose a trace-optional framework that assesses token efficiency by evaluating completion, conditional correctness, verbosity, and other factors, with optional use of benchmark metadata and reasoning traces to provide more detailed insights.
Results: Results: The evaluation of 25 models on the CogniLoad dataset reveals that accuracy and token-efficiency rankings diverge, with efficiency gaps often driven by conditional correctness, and verbalization overhead varying significantly across models.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### MILE-RefHumEval: A Reference-Free, Multi-Independent LLM Framework for Human-Aligned Evaluation (2602.09624v1)

Authors: Nalin Srun, Parisa Rastin, Guénaël Cabanes, et al.
Published: 2026-02-10
Updated: 2026-02-10
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.09624v1)

Abstract: We introduce MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination. It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement. With task-specific prompts from best candidate selection, summarization and image captioning to dialogue, MILE-RefHumEval provides flexible, interpretable, and scalable assessments. Experiments show it aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.

Summary:
Purpose: The paper introduces MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination.
Method: It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement with task-specific prompts.
Results: Experiments show that MILE-RefHumEval aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.
Tags: Judge Reliability And Calibration, Multi-Judge Or Ensemble Methods

### BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation (2602.09383v1)

Authors: Peng Lai, Zhihao Ou, Yong Wang, et al.
Published: 2026-02-10
Updated: 2026-02-10
Categories: cs.CL, cs.AI, cs.SE
Link: [arXiv](http://arxiv.org/abs/2602.09383v1)

Abstract: LLM-as-a-Judge has been widely adopted across various research and practical applications, yet the robustness and reliability of its evaluation remain a critical issue. A core challenge it faces is bias, which has primarily been studied in terms of known biases and their impact on evaluation outcomes, while automated and systematic exploration of potential unknown biases is still lacking. Nevertheless, such exploration is crucial for enhancing the robustness and reliability of evaluations. To bridge this gap, we propose BiasScope, a LLM-driven framework for automatically and at scale discovering potential biases that may arise during model evaluation. BiasScope can uncover potential biases across different model families and scales, with its generality and effectiveness validated on the JudgeBench dataset. It overcomes the limitations of existing approaches, transforming bias discovery from a passive process relying on manual effort and predefined bias lists into an active and comprehensive automated exploration. Moreover, based on BiasScope, we propose JudgeBench-Pro, an extended version of JudgeBench and a more challenging benchmark for evaluating the robustness of LLM-as-a-judge. Strikingly, even powerful LLMs as evaluators show error rates above 50\% on JudgeBench-Pro, underscoring the urgent need to strengthen evaluation robustness and to mitigate potential biases further.

Summary:
Purpose: The paper aims to address the critical issue of bias in LLM-as-a-Judge evaluation by proposing a framework for automated detection of potential biases that may arise during model evaluation.
Method: The proposed framework, called BiasScope, is a LLM-driven approach that can automatically and at scale discover potential biases across different model families and scales, overcoming the limitations of existing manual and predefined bias-based approaches.
Results: The effectiveness of BiasScope is validated on the JudgeBench dataset, and its application leads to the creation of JudgeBench-Pro, a more challenging benchmark that reveals error rates above 50% for even powerful LLMs as evaluators, highlighting the need for strengthened evaluation robustness.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### LingxiDiagBench: A Multi-Agent Framework for Benchmarking LLMs in Chinese Psychiatric Consultation and Diagnosis (2602.09379v2)

Authors: Shihao Xu, Tiancheng Zhou, Jiatong Ma, et al.
Published: 2026-02-10
Updated: 2026-02-11
Categories: cs.MA, cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.09379v2)

Abstract: Mental disorders are highly prevalent worldwide, but the shortage of psychiatrists and the inherent subjectivity of interview-based diagnosis create substantial barriers to timely and consistent mental-health assessment. Progress in AI-assisted psychiatric diagnosis is constrained by the absence of benchmarks that simultaneously provide realistic patient simulation, clinician-verified diagnostic labels, and support for dynamic multi-turn consultation. We present LingxiDiagBench, a large-scale multi-agent benchmark that evaluates LLMs on both static diagnostic inference and dynamic multi-turn psychiatric consultation in Chinese. At its core is LingxiDiag-16K, a dataset of 16,000 EMR-aligned synthetic consultation dialogues designed to reproduce real clinical demographic and diagnostic distributions across 12 ICD-10 psychiatric categories. Through extensive experiments across state-of-the-art LLMs, we establish key findings: (1) although LLMs achieve high accuracy on binary depression--anxiety classification (up to 92.3%), performance deteriorates substantially for depression--anxiety comorbidity recognition (43.0%) and 12-way differential diagnosis (28.5%); (2) dynamic consultation often underperforms static evaluation, indicating that ineffective information-gathering strategies significantly impair downstream diagnostic reasoning; (3) consultation quality assessed by LLM-as-a-Judge shows only moderate correlation with diagnostic accuracy, suggesting that well-structured questioning alone does not ensure correct diagnostic decisions. We release LingxiDiag-16K and the full evaluation framework to support reproducible research at https://github.com/Lingxi-mental-health/LingxiDiagBench.

Summary:
Purpose: The paper aims to address the challenges in AI-assisted psychiatric diagnosis by introducing a benchmark that provides realistic patient simulation, clinician-verified diagnostic labels, and support for dynamic multi-turn consultation.
Method: The authors present LingxiDiagBench, a large-scale multi-agent benchmark that evaluates LLMs on both static diagnostic inference and dynamic multi-turn psychiatric consultation in Chinese, utilizing a dataset of 16,000 EMR-aligned synthetic consultation dialogues.
Results: The experiments establish key findings, including the varying performance of LLMs across different diagnostic tasks, the impact of ineffective information-gathering strategies on diagnostic reasoning, and the moderate correlation between consultation quality and diagnostic accuracy.
Tags: Benchmark And Dataset Creation, Domain-Specific Judging
