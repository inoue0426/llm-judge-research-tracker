# LLM as a Judge — Daily Report

Report date: 2026-02-17 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 6

## Papers

### Multi-Objective Alignment of Language Models for Personalized Psychotherapy (2602.16053v1)

Authors: Mehrab Beikzadeh, Yasaman Asadollah Salmanpour, Ashima Suvarna, et al.
Published: 2026-02-17
Updated: 2026-02-17
Categories: cs.LG, cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.16053v1)

Abstract: Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.

Summary:
Purpose: The purpose of this study is to develop a multi-objective alignment framework for language models in personalized psychotherapy to balance patient preferences with clinical safety.
Method: The method used involves surveying individuals with lived mental health experience, collecting preference rankings, and training reward models for six therapeutic criteria using direct preference optimization.
Results: The results show that the proposed multi-objective approach, MODPO, achieves a superior balance between empathy and safety, outperforming single-objective optimization and other methods, and is consistently preferred by blinded clinician evaluation.
Tags: Judge Reliability And Calibration, Judge Prompting Protocols

### *-PLUIE: Personalisable metric with Llm Used for Improved Evaluation (2602.15778v1)

Authors: Quentin Lemesle, Léane Jourdan, Daisy Munson, et al.
Published: 2026-02-17
Updated: 2026-02-17
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.15778v1)

Abstract: Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.

Summary:
Purpose: The purpose of this paper is to address the limitations of existing LLM-as-a-judge methods for evaluating automatically generated text, which are computationally expensive and require post-processing.
Method: Method: The researchers build upon ParaPLUIE, a perplexity-based LLM-judge metric, and introduce *-PLUIE, task-specific prompting variants that estimate confidence over "Yes/No" answers without generating text.
Results: Results: The experiments demonstrate that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost, making it a more effective and efficient evaluation method.
Tags: Judge Reliability And Calibration, Metrics And Scoring Methods

### ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models (2602.15758v1)

Authors: Manav Nitin Kapadnis, Lawanya Baghel, Atharva Naik, et al.
Published: 2026-02-17
Updated: 2026-02-17
Categories: cs.CL, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.15758v1)

Abstract: While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.

Summary:
Purpose: The paper introduces ChartEditBench, a benchmark for evaluating the ability of Multimodal Large Language Models (MLLMs) to support real-world exploratory data analysis through multi-turn chart editing interactions.
Method: The benchmark comprises 5,000 difficulty-controlled modification chains and a rigorously human-verified subset, and uses a robust evaluation framework that combines execution-based fidelity checks, pixel-level visual similarity, and logical code verification.
Results: Experiments with state-of-the-art MLLMs using ChartEditBench reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, highlighting the need for improved grounded multimodal programming capabilities.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### Quantifying construct validity in large language model evaluations (2602.15532v1)

Authors: Ryan Othniel Kearns
Published: 2026-02-17
Updated: 2026-02-17
Categories: cs.AI, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.15532v1)

Abstract: The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance. Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks. This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.

Summary:
Purpose: The paper aims to address the issue of construct validity in large language model (LLM) evaluations by developing a method to separate benchmark results from actual capabilities.
Method: The study proposes a structured capabilities model that combines the strengths of latent factor models and scaling laws to extract interpretable and generalisable capabilities from LLM benchmark results.
Results: The structured capabilities model outperforms existing approaches, demonstrating better explanatory and predictive power for quantifying construct validity in LLM evaluations through improved parsimonious fit indices and out-of-distribution benchmark prediction.
Tags: Benchmark And Dataset Creation, Metrics And Scoring Methods

### SecCodeBench-V2 Technical Report (2602.15485v2)

Authors: Longfei Chen, Ji Zhao, Lanxiao Cui, et al.
Published: 2026-02-17
Updated: 2026-02-18
Categories: cs.CR, cs.AI, cs.SE
Link: [arXiv](http://arxiv.org/abs/2602.15485v2)

Abstract: We introduce SecCodeBench-V2, a publicly released benchmark for evaluating Large Language Model (LLM) copilots' capabilities of generating secure code. SecCodeBench-V2 comprises 98 generation and fix scenarios derived from Alibaba Group's industrial productions, where the underlying security issues span 22 common CWE (Common Weakness Enumeration) categories across five programming languages: Java, C, Python, Go, and JavaScript. SecCodeBench-V2 adopts a function-level task formulation: each scenario provides a complete project scaffold and requires the model to implement or patch a designated target function under fixed interfaces and dependencies. For each scenario, SecCodeBench-V2 provides executable proof-of-concept (PoC) test cases for both functional validation and security verification. All test cases are authored and double-reviewed by security experts, ensuring high fidelity, broad coverage, and reliable ground truth. Beyond the benchmark itself, we build a unified evaluation pipeline that assesses models primarily via dynamic execution. For most scenarios, we compile and run model-generated artifacts in isolated environments and execute PoC test cases to validate both functional correctness and security properties. For scenarios where security issues cannot be adjudicated with deterministic test cases, we additionally employ an LLM-as-a-judge oracle. To summarize performance across heterogeneous scenarios and difficulty levels, we design a Pass@K-based scoring protocol with principled aggregation over scenarios and severity, enabling holistic and comparable evaluation across models. Overall, SecCodeBench-V2 provides a rigorous and reproducible foundation for assessing the security posture of AI coding assistants, with results and artifacts released at https://alibaba.github.io/sec-code-bench. The benchmark is publicly available at https://github.com/alibaba/sec-code-bench.

Summary:
Purpose: The paper introduces SecCodeBench-V2, a publicly released benchmark for evaluating the capabilities of Large Language Model copilots in generating secure code across various programming languages and security issues.
Method: The benchmark comprises 98 generation and fix scenarios with function-level task formulation, providing executable proof-of-concept test cases for functional validation and security verification, and adopts a unified evaluation pipeline assessing models via dynamic execution.
Results: SecCodeBench-V2 enables holistic and comparable evaluation of AI coding assistants' security posture through a Pass@K-based scoring protocol, providing a rigorous and reproducible foundation with results and artifacts publicly available.
Tags: Benchmark And Dataset Creation, Metrics And Scoring Methods

### LLM-as-Judge on a Budget (2602.15481v1)

Authors: Aadirupa Saha, Aniket Wagde, Branislav Kveton
Published: 2026-02-17
Updated: 2026-02-17
Categories: cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.15481v1)

Abstract: LLM-as-a-judge has emerged as a cornerstone technique for evaluating large language models by leveraging LLM reasoning to score prompt-response pairs. Since LLM judgments are stochastic, practitioners commonly query each pair multiple times to estimate mean scores accurately. This raises a critical challenge: given a fixed computational budget $B$, how to optimally allocate queries across $K$ prompt-response pairs to minimize estimation error? % We present a principled variance-adaptive approach leveraging multi-armed bandit theory and concentration inequalities. Our method dynamically allocates queries based on estimated score variances, concentrating resources where uncertainty is highest. Further, our algorithm is shown to achieve a worst-case score-estimation error of $\tilde{O}\left(\sqrt{\frac{\sum_{i=1}^K σ_i^2}{B}}\right)$, $σ_i^2$ being the unknown score variance for pair $i \in [K]$ with near-optimal budget allocation. % Experiments on \emph{Summarize-From-Feedback} and \emph{HelpSteer2} demonstrate that our method significantly outperforms uniform allocation, reducing worst-case estimation error while maintaining identical budgets. Our work establishes a theoretical foundation for efficient LLM evaluation with practical implications for AI safety, model alignment, and automated assessment at scale.

Summary:
Purpose: The paper aims to address the challenge of optimally allocating queries across prompt-response pairs to minimize estimation error when using large language models as judges, given a fixed computational budget.
Method: The approach presented in the paper leverages multi-armed bandit theory and concentration inequalities to dynamically allocate queries based on estimated score variances, concentrating resources where uncertainty is highest.
Results: The proposed method achieves a significant reduction in worst-case estimation error compared to uniform allocation, while maintaining identical budgets, as demonstrated through experiments on Summarize-From-Feedback and HelpSteer2 datasets.
Tags: Benchmark And Dataset Creation, Metrics And Scoring Methods
