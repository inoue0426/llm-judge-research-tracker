# LLM as a Judge â€” Daily Report

Report date: 2026-02-11 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 6

## Papers

### Advancing AI Trustworthiness Through Patient Simulation: Risk Assessment of Conversational Agents for Antidepressant Selection (2602.11391v1)

Authors: Md Tanvir Rouf Shawon, Mohammad Sabik Irbaz, Hadeel R. A. Elyazori, et al.
Published: 2026-02-11
Updated: 2026-02-11
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.11391v1)

Abstract: Objective: This paper introduces a patient simulator designed to enable scalable, automated evaluation of healthcare conversational agents. The simulator generates realistic, controllable patient interactions that systematically vary across medical, linguistic, and behavioral dimensions, allowing annotators and an independent AI judge to assess agent performance, identify hallucinations and inaccuracies, and characterize risk patterns across diverse patient populations. Methods: The simulator is grounded in the NIST AI Risk Management Framework and integrates three profile components reflecting different dimensions of patient variation: (1) medical profiles constructed from electronic health records in the All of Us Research Program; (2) linguistic profiles modeling variation in health literacy and condition-specific communication patterns; and (3) behavioral profiles representing empirically observed interaction patterns, including cooperation, distraction, and adversarial engagement. We evaluated the simulator's effectiveness in identifying errors in an AI decision aid for antidepressant selection. Results: We generated 500 conversations between the patient simulator and the AI decision aid across systematic combinations of five linguistic and three behavioral profiles. Human annotators assessed 1,787 medical concepts across 100 conversations, achieving high agreement (F1=0.94, \k{appa}=0.73), and the LLM judge achieved comparable agreement with human annotators (F1=0.94, \k{appa}=0.78; paired bootstrap p=0.21). The simulator revealed a monotonic degradation in AI decision aid performance across the health literacy spectrum: rank-one concept retrieval accuracy increased from 47.9% for limited health literacy to 69.1% for functional and 81.6% for proficient.

Summary:
Purpose: This paper introduces a patient simulator designed to enable scalable, automated evaluation of healthcare conversational agents through realistic and controllable patient interactions.
Method: The simulator is grounded in the NIST AI Risk Management Framework and integrates three profile components reflecting different dimensions of patient variation, including medical, linguistic, and behavioral profiles.
Results: The simulator effectively identified errors in an AI decision aid for antidepressant selection, revealing a degradation in performance across the health literacy spectrum with rank-one concept retrieval accuracy increasing from 47.9% to 81.6% as health literacy improved.
Tags: Judge Reliability And Calibration, Robustness And Sensitivity

### Bi-Level Prompt Optimization for Multimodal LLM-as-a-Judge (2602.11340v1)

Authors: Bo Pan, Xuan Kan, Kaitai Zhang, et al.
Published: 2026-02-11
Updated: 2026-02-11
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.11340v1)

Abstract: Large language models (LLMs) have become widely adopted as automated judges for evaluating AI-generated content. Despite their success, aligning LLM-based evaluations with human judgments remains challenging. While supervised fine-tuning on human-labeled data can improve alignment, it is costly and inflexible, requiring new training for each task or dataset. Recent progress in auto prompt optimization (APO) offers a more efficient alternative by automatically improving the instructions that guide LLM judges. However, existing APO methods primarily target text-only evaluations and remain underexplored in multimodal settings. In this work, we study auto prompt optimization for multimodal LLM-as-a-judge, particularly for evaluating AI-generated images. We identify a key bottleneck: multimodal models can only process a limited number of visual examples due to context window constraints, which hinders effective trial-and-error prompt refinement. To overcome this, we propose BLPO, a bi-level prompt optimization framework that converts images into textual representations while preserving evaluation-relevant visual cues. Our bi-level optimization approach jointly refines the judge prompt and the I2T prompt to maintain fidelity under limited context budgets. Experiments on four datasets and three LLM judges demonstrate the effectiveness of our method.

Summary:
Purpose: The purpose of this work is to improve the alignment of large language model (LLM) evaluations with human judgments, particularly in multimodal settings where AI-generated images are evaluated.
Method: The method proposed in this paper, called Bi-Level Prompt Optimization (BLPO), converts images into textual representations and jointly refines the judge prompt and the image-to-text (I2T) prompt to maintain fidelity under limited context budgets.
Results: The results of experiments on four datasets and three LLM judges demonstrate the effectiveness of the BLPO framework in evaluating AI-generated images, offering a more efficient alternative to supervised fine-tuning on human-labeled data.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### CryptoAnalystBench: Failures in Multi-Tool Long-Form LLM Analysis (2602.11304v1)

Authors: Anushri Eswaran, Oleg Golev, Darshan Tank, et al.
Published: 2026-02-11
Updated: 2026-02-11
Categories: cs.CR, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.11304v1)

Abstract: Modern analyst agents must reason over complex, high token inputs, including dozens of retrieved documents, tool outputs, and time sensitive data. While prior work has produced tool calling benchmarks and examined factuality in knowledge augmented systems, relatively little work studies their intersection: settings where LLMs must integrate large volumes of dynamic, structured and unstructured multi tool outputs. We investigate LLM failure modes in this regime using crypto as a representative high data density domain. We introduce (1) CryptoAnalystBench, an analyst aligned benchmark of 198 production crypto and DeFi queries spanning 11 categories; (2) an agentic harness equipped with relevant crypto and DeFi tools to generate responses across multiple frontier LLMs; and (3) an evaluation pipeline with citation verification and an LLM as a judge rubric spanning four user defined success dimensions: relevance, temporal relevance, depth, and data consistency. Using human annotation, we develop a taxonomy of seven higher order error types that are not reliably captured by factuality checks or LLM based quality scoring. We find that these failures persist even in state of the art systems and can compromise high stakes decisions. Based on this taxonomy, we refine the judge rubric to better capture these errors. While the judge does not align with human annotators on precise scoring across rubric iterations, it reliably identifies critical failure modes, enabling scalable feedback for developers and researchers studying analyst style agents. We release CryptoAnalystBench with annotated queries, the evaluation pipeline, judge rubrics, and the error taxonomy, and outline mitigation strategies and open challenges in evaluating long form, multi tool augmented systems.

Summary:
Purpose: The paper investigates the failure modes of large language models (LLMs) when analyzing complex, high-token inputs from multiple tools in a representative domain, specifically crypto and DeFi queries.
Method: The authors introduce CryptoAnalystBench, a benchmark of 198 production crypto and DeFi queries, an agentic harness to generate responses across multiple LLMs, and an evaluation pipeline with citation verification and a judge rubric to assess the quality of the responses.
Results: The study finds that even state-of-the-art systems exhibit persistent failures that can compromise high-stakes decisions, and proposes a refined judge rubric and error taxonomy to better capture these errors, enabling scalable feedback for developers and researchers.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight (2602.11136v2)

Authors: Jiayi Zhou, Yang Sheng, Hantao Lou, et al.
Published: 2026-02-11
Updated: 2026-02-12
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.11136v2)

Abstract: As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.

Summary:
Purpose: The paper aims to address the dilemma of ensuring behavioral safety in large language model (LLM)-based agents operating in high-stakes domains by proposing a neuro-symbolic framework for agentic oversight.
Method: The proposed framework, FormalJudge, employs a bidirectional Formal-of-Thought architecture that uses LLMs as specification compilers to translate natural language requirements into formal specifications and verify compliance using Dafny and Z3 solvers.
Results: The experiments demonstrate that FormalJudge achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization, and provides near-linear safety improvement through iterative refinement across three benchmarks and seven agent models.
Tags: Metrics And Scoring Methods, Benchmark And Dataset Creation

### In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution (2602.11079v2)

Authors: Frank Xiao, Santiago Aranguri
Published: 2026-02-11
Updated: 2026-02-13
Categories: cs.LG, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.11079v2)

Abstract: We propose activation-based data attribution, a method that traces behavioral changes in post-trained language models to responsible training datapoints. By computing activation-difference vectors for both test prompts and preference pairs and ranking by cosine similarity, we identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data. Clustering behavior-datapoint similarity matrices also enables unsupervised discovery of emergent behaviors. Applying this to OLMo 2's production DPO training, we surfaced distractor-triggered compliance: a harmful behavior where the model complies with dangerous requests when benign formatting instructions are appended. Filtering top-ranked datapoints reduces this behavior by 63% while switching their labels achieves 78%. Our method outperforms gradient-based attribution and LLM-judge baselines while being over 10 times cheaper than both. This in-the-wild model organism - emerging from contaminated preference data rather than deliberate injection - provides a realistic benchmark for safety techniques.

Summary:
Purpose: The paper proposes a method to mitigate undesirable emergent behaviors in production language models by tracing behavioral changes to responsible training datapoints using activation-based data attribution.
Method: By computing activation-difference vectors and ranking by cosine similarity, the authors identify datapoints that cause specific behaviors and validate these attributions causally by retraining with modified data.
Results: The proposed method is shown to be effective in reducing harmful behaviors, such as distractor-triggered compliance, by up to 78% while outperforming baseline methods at a lower computational cost.
Tags: Benchmark And Dataset Creation

### S-GRec: Personalized Semantic-Aware Generative Recommendation with Asymmetric Advantage (2602.10606v2)

Authors: Jie Jiang, Hongbo Tang, Wenjie Wu, et al.
Published: 2026-02-11
Updated: 2026-02-12
Categories: cs.IR
Link: [arXiv](http://arxiv.org/abs/2602.10606v2)

Abstract: Generative recommendation models sequence generation to produce items end-to-end, but training from behavioral logs often provides weak supervision on underlying user intent. Although Large Language Models (LLMs) offer rich semantic priors that could supply such supervision, direct adoption in industrial recommendation is hindered by two obstacles: semantic signals can conflict with platform business objectives, and LLM inference is prohibitively expensive at scale. This paper presents S-GRec, a semantic-aware framework that decouples an online lightweight generator from an offline LLM-based semantic judge for train-time supervision. S-GRec introduces a two-stage Personalized Semantic Judge (PSJ) that produces interpretable aspect evidence and learns user-conditional aggregation from pairwise feedback, yielding stable semantic rewards. To prevent semantic supervision from deviating from business goals, Asymmetric Advantage Policy Optimization (A2PO) anchors optimization on business rewards (e.g., eCPM) and injects semantic advantages only when they are consistent. Extensive experiments on public benchmarks and a large-scale production system validate both effectiveness and scalability, including statistically significant gains in CTR and a 1.19\% lift in GMV in online A/B tests, without requiring real-time LLM inference.

Summary:
Purpose: The purpose of this paper is to present S-GRec, a semantic-aware framework that addresses the limitations of generative recommendation models by incorporating Large Language Models (LLMs) to provide rich semantic priors for training.
Method: Methodologically, S-GRec decouples an online lightweight generator from an offline LLM-based semantic judge and introduces a two-stage Personalized Semantic Judge (PSJ) to produce interpretable aspect evidence and learn user-conditional aggregation from pairwise feedback.
Results: Results from extensive experiments on public benchmarks and a large-scale production system demonstrate the effectiveness and scalability of S-GRec, including statistically significant gains in CTR and a 1.19% lift in GMV in online A/B tests.
Tags: Judge Prompting Protocols, Benchmark And Dataset Creation
