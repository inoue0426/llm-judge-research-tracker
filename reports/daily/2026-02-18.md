# LLM as a Judge — Daily Report

Report date: 2026-02-18 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 7

## Papers

### SourceBench: Can AI Answers Reference Quality Web Sources? (2602.16942v1)

Authors: Hexi Jin, Stephen Liu, Yuheng Li, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.16942v1)

Abstract: Large language models (LLMs) increasingly answer queries by citing web sources, but existing evaluations emphasize answer correctness rather than evidence quality. We introduce SourceBench, a benchmark for measuring the quality of cited web sources across 100 real-world queries spanning informational, factual, argumentative, social, and shopping intents. SourceBench uses an eight-metric framework covering content quality (content relevance, factual accuracy, objectivity) and page-level signals (e.g., freshness, authority/accountability, clarity), and includes a human-labeled dataset with a calibrated LLM-based evaluator that matches expert judgments closely. We evaluate eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench and conduct further experiments to understand the evaluation results. Overall, our work reveals four key new insights that can guide future research in the direction of GenAI and web search.

Summary:
Purpose: The paper introduces SourceBench, a benchmark for measuring the quality of cited web sources used by large language models (LLMs) to answer queries, with the goal of evaluating evidence quality rather than just answer correctness.
Method: The SourceBench framework uses eight metrics to assess content quality and page-level signals, and includes a human-labeled dataset with a calibrated LLM-based evaluator to evaluate the cited sources.
Results: The evaluation of eight LLMs, Google Search, and three AI search tools over 3996 cited sources using SourceBench reveals four key new insights that can guide future research in GenAI and web search.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark (2602.16811v1)

Authors: Charalampos Mastrokostas, Nikolaos Giarelis, Nikos Karacapilidis
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.CL, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.16811v1)

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

Summary:
Purpose: The study aims to address the research gap in Greek Question Answering by evaluating the effectiveness of monolingual and multilingual Large Language Models on language-specific tasks.
Method: The evaluation is conducted using DemosQA, a novel dataset constructed from social media user questions and community-reviewed answers, as well as a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages.
Results: The study presents an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies, with the code and data released to facilitate reproducibility.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### References Improve LLM Alignment in Non-Verifiable Domains (2602.16802v1)

Authors: Kejian Shi, Yixin Liu, Peifeng Wang, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.CL, cs.AI, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.16802v1)

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

Summary:
Purpose: The paper investigates whether reference-guided LLM-evaluators can bridge the gap in non-verifiable domains by serving as soft "verifiers" for LLM alignment, where Reinforcement Learning with Verifiable Rewards cannot be directly applied.
Method: The researchers design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs and conduct comprehensive experiments to test the effectiveness of a reference-guided approach.
Results: The study shows that a reference-guided approach substantially improves the accuracy of LLM-judges, yielding clear gains over direct SFT on reference outputs and self-improvement with reference-free judges, and achieving performance comparable to training with strong finetuned reward models.
Tags: Judge Reliability And Calibration

### Who can we trust? LLM-as-a-jury for Comparative Assessment (2602.16610v1)

Authors: Mengjie Qian, Guangzhi Sun, Mark J. F. Gales, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.CL, cs.AI, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.16610v1)

Abstract: Large language models (LLMs) are increasingly applied as automatic evaluators for natural language generation assessment often using pairwise comparative judgements. Existing approaches typically rely on single judges or aggregate multiple judges assuming equal reliability. In practice, LLM judges vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent. Furthermore, human-labelled supervision for judge calibration may be unavailable. We first empirically demonstrate that inconsistencies in LLM comparison probabilities exist and show that it limits the effectiveness of direct probability-based ranking. To address this, we study the LLM-as-a-jury setting and propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. Experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods, and that the learned discriminator strongly correlates with independent measures of the cycle consistency of LLM judgments. Further analysis reveals that BT-sigma can be interpreted as an unsupervised calibration mechanism that improves aggregation by modelling judge reliability.

Summary:
Purpose: The purpose of this paper is to assess the reliability of large language models (LLMs) as automatic evaluators for natural language generation assessment, particularly in pairwise comparative judgements.
Method: Methodologically, the authors propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a discriminator parameter for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone.
Results: Results of experiments on benchmark NLG evaluation datasets show that BT-sigma consistently outperforms averaging-based aggregation methods and effectively models judge reliability through an unsupervised calibration mechanism.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes (2602.16607v1)

Authors: Miguel Marques, Ana Luísa Fernandes, Ana Filipa Pacheco, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.16607v1)

Abstract: Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.

Summary:
Purpose: The purpose of this paper is to address the challenge of navigating lengthy and dense municipal meeting minutes by presenting a new corpus called CitiLink-Summ for automatic summarization of discussion subjects in European Portuguese.
Method: Methodologically, the authors leverage the CitiLink-Summ dataset, comprising 100 documents and 2,322 manually crafted summaries, to establish baseline results using state-of-the-art generative models and large language models evaluated with various metrics.
Results: Results from this study provide the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts through the establishment of baseline results for automatic summarization.
Tags: Benchmark And Dataset Creation, Metrics And Scoring Methods

### Toward Scalable Verifiable Reward: Proxy State-Based Evaluation for Multi-turn Tool-Calling LLM Agents (2602.16246v1)

Authors: Yun-Shiuan Chuang, Chaitanya Kulkarni, Alec Chiu, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.16246v1)

Abstract: Interactive large language model (LLM) agents operating via multi-turn dialogue and multi-step tool calling are increasingly used in production. Benchmarks for these agents must both reliably compare models and yield on-policy training data. Prior agentic benchmarks (e.g., tau-bench, tau2-bench, AppWorld) rely on fully deterministic backends, which are costly to build and iterate. We propose Proxy State-Based Evaluation, an LLM-driven simulation framework that preserves final state-based evaluation without a deterministic database. Specifically, a scenario specifies the user goal, user/system facts, expected final state, and expected agent behavior, and an LLM state tracker infers a structured proxy state from the full interaction trace. LLM judges then verify goal completion and detect tool/user hallucinations against scenario constraints. Empirically, our benchmark produces stable, model-differentiating rankings across families and inference-time reasoning efforts, and its on-/off-policy rollouts provide supervision that transfers to unseen scenarios. Careful scenario specification yields near-zero simulator hallucination rates as supported by ablation studies. The framework also supports sensitivity analyses over user personas. Human-LLM judge agreement exceeds 90%, indicating reliable automated evaluation. Overall, proxy state-based evaluation offers a practical, scalable alternative to deterministic agentic benchmarks for industrial LLM agents.

Summary:
Purpose: The paper aims to propose a novel evaluation framework for multi-turn tool-calling large language model agents that can reliably compare models and yield on-policy training data in a scalable manner.
Method: The proposed approach, called Proxy State-Based Evaluation, utilizes an LLM-driven simulation framework that preserves final state-based evaluation without requiring a deterministic database, instead relying on scenario specifications and LLM state trackers to infer proxy states.
Results: The empirical results show that the benchmark produces stable and model-differentiating rankings, provides effective supervision for unseen scenarios, and achieves high human-LLM judge agreement, indicating a reliable and scalable alternative to traditional deterministic agentic benchmarks.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### LiveClin: A Live Clinical Benchmark without Leakage (2602.16747v1)

Authors: Xidong Wang, Shuqi Guo, Yue Shen, et al.
Published: 2026-02-18
Updated: 2026-02-18
Categories: cs.LG, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.16747v1)

Abstract: The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for approximating real-world clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. In benchmarking against human experts, Chief Physicians achieved the highest accuracy, followed closely by Attending Physicians, with both surpassing most models. LiveClin thus provides a continuously evolving, clinically grounded framework to guide the development of medical LLMs towards closing this gap and achieving greater reliability and real-world utility. Our data and code are publicly available at https://github.com/AQ-MedAI/LiveClin.

Summary:
Purpose: The paper introduces LiveClin, a live clinical benchmark designed to address the challenges of data contamination and knowledge obsolescence in evaluating medical large language models (LLMs).
Method: Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin uses a verified AI-human workflow involving physicians to transform authentic patient cases into complex evaluation scenarios.
Results: The evaluation of 26 models on LiveClin reveals the difficulty of real-world clinical scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%, and human experts, particularly Chief Physicians, surpassing most models in accuracy.
Tags: Judge Reliability And Calibration, Domain-Specific Judging
