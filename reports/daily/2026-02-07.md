# LLM as a Judge â€” Daily Report

Report date: 2026-02-07 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 1

## Papers

### Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation (2602.07673v1)

Authors: Jiangnan Fang, Cheng-Tse Liu, Hanieh Deilamsalehy, et al.
Published: 2026-02-07
Updated: 2026-02-07
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.07673v1)

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

Summary:
Purpose: The purpose of this study is to analyze the bias of large language model (LLM) judges in evaluating summaries, particularly in relation to their overlap with human-written responses.
Method: Methodologically, the researchers test 9 recent LLMs with varying parameter counts using a well-defined overlap metric, including ROUGE and BLEU, to assess their preferences for summaries generated by other LLMs versus those written by humans.
Results: Results show that LLM judges increasingly prefer summaries generated by other LLMs over human-written ones as the similarities between judged summaries decrease, suggesting that LLM-as-a-judge should rely on techniques beyond simple comparison.
Tags: Judge Reliability And Calibration, Robustness And Sensitivity
