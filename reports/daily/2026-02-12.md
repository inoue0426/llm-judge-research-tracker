# LLM as a Judge — Daily Report

Report date: 2026-02-12 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 4

## Papers

### RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty (2602.12424v1)

Authors: Ziqian Zhang, Xingjian Hu, Yue Huang, et al.
Published: 2026-02-12
Updated: 2026-02-12
Categories: cs.CL, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.12424v1)

Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.

Summary:
Purpose: The paper proposes a novel framework called RankLLM to quantify both question difficulty and model competency in evaluating large language models (LLMs), addressing the limitation of existing benchmarks that fail to differentiate question difficulty.
Method: The core mechanism of RankLLM facilitates bidirectional score propagation between models and questions, where a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model.
Results: RankLLM achieves 90% agreement with human judgments, outperforms strong baselines, and exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation

### LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss (2602.12005v2)

Authors: Szilvia Ujváry, Louis Béthune, Pierre Ablin, et al.
Published: 2026-02-12
Updated: 2026-02-13
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.12005v2)

Abstract: Language models have consistently grown to compress more world knowledge into their parameters, but the knowledge that can be pretrained into them is upper-bounded by their parameter size. Especially the capacity of Small Language Models (SLMs) is limited, leading to factually incorrect generations. This problem is often mitigated by giving the SLM access to an outside source: the ability to query a larger model, documents, or a database. Under this setting, we study the fundamental question of \emph{which tokens an SLM can and should learn} during pretraining, versus \emph{which ones it should delegate} via a \texttt{<CALL>} token. We find that this is not simply a question of loss: although the loss is predictive of whether a predicted token mismatches the ground-truth, some tokens are \emph{acceptable} in that they are truthful alternative continuations of a pretraining document, and should not trigger a \texttt{<CALL>} even if their loss is high. We find that a spaCy grammar parser can help augment the loss signal to decide which tokens the SLM should learn to delegate to prevent factual errors and which are safe to learn and predict even under high losses. We propose LaCy, a novel pretraining method based on this token selection philosophy. Our experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help. This results in higher FactScores when generating in a cascade with a bigger model and outperforms Rho or LLM-judge trained SLMs, while being simpler and cheaper.

Summary:
Purpose: The paper investigates the fundamental question of what small language models can and should learn during pretraining, versus what they should delegate to an outside source to prevent factual errors.
Method: The researchers propose a novel pretraining method called LaCy, which uses a spaCy grammar parser to augment the loss signal and decide which tokens the model should learn to delegate.
Results: The experiments demonstrate that LaCy models successfully learn which tokens to predict and where to delegate for help, resulting in higher FactScores and outperforming other methods while being simpler and cheaper.
Tags: Metrics And Scoring Methods

### LLM-based Triplet Extraction from Financial Reports (2602.11886v1)

Authors: Dante Wesslund, Ville Stenström, Pontus Linde, et al.
Published: 2026-02-12
Updated: 2026-02-12
Categories: cs.CL
Link: [arXiv](http://arxiv.org/abs/2602.11886v1)

Abstract: Corporate financial reports are a valuable source of structured knowledge for Knowledge Graph construction, but the lack of annotated ground truth in this domain makes evaluation difficult. We present a semi-automated pipeline for Subject-Predicate-Object triplet extraction that uses ontology-driven proxy metrics, specifically Ontology Conformance and Faithfulness, instead of ground-truth-based evaluation. We compare a static, manually engineered ontology against a fully automated, document-specific ontology induction approach across different LLMs and two corporate annual reports. The automatically induced ontology achieves 100% schema conformance in all configurations, eliminating the ontology drift observed with the manual approach. We also propose a hybrid verification strategy that combines regex matching with an LLM-as-a-judge check, reducing apparent subject hallucination rates from 65.2% to 1.6% by filtering false positives caused by coreference resolution. Finally, we identify a systematic asymmetry between subject and object hallucinations, which we attribute to passive constructions and omitted agents in financial prose.

Summary:
Purpose: The paper aims to present a semi-automated pipeline for extracting Subject-Predicate-Object triplets from corporate financial reports using Large Language Models (LLMs) and ontology-driven proxy metrics for evaluation.
Method: The approach uses a combination of static and automated ontology induction methods, as well as a hybrid verification strategy that combines regex matching with LLM-based judgment to extract accurate triplets from financial reports.
Results: The automatically induced ontology achieves 100% schema conformance and reduces subject hallucination rates from 65.2% to 1.6%, while also revealing a systematic asymmetry between subject and object hallucinations in financial prose.
Tags: Metrics And Scoring Methods

### Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing (2602.11786v1)

Authors: Keita Broadwater
Published: 2026-02-12
Updated: 2026-02-12
Categories: cs.LG, cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.11786v1)

Abstract: Traditional benchmarks for large language models (LLMs) primarily assess safety risk through breadth-oriented evaluation across diverse tasks. However, real-world deployment exposes a different class of risk: operational failures arising from repeated inference on identical or near-identical prompts rather than broad task generalization. In high-stakes settings, response consistency and safety under sustained use are critical. We introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework inspired by reliability engineering. APST repeatedly samples identical prompts under controlled operational conditions (e.g., decoding temperature) to surface latent failure modes including hallucinations, refusal inconsistency, and unsafe completions. Rather than treating failures as isolated events, APST models them as stochastic outcomes of independent inference events. We formalize safety failures using Bernoulli and binomial models to estimate per-inference failure probabilities, enabling quantitative comparison of reliability across models and decoding configurations. Applying APST to multiple instruction-tuned LLMs evaluated on AIR-BENCH-derived safety prompts, we find that models with similar benchmark-aligned scores can exhibit substantially different empirical failure rates under repeated sampling, particularly as temperature increases. These results demonstrate that shallow, single-sample evaluation can obscure meaningful reliability differences under sustained use. APST complements existing benchmarks by providing a practical framework for evaluating LLM safety and reliability under repeated inference, bridging benchmark alignment and deployment-oriented risk assessment.

Summary:
Purpose: The paper aims to evaluate the safety of large language models (LLMs) under repeated inference, which is a critical aspect in high-stakes settings where response consistency and safety are essential.
Method: The researchers introduce Accelerated Prompt Stress Testing (APST), a depth-oriented evaluation framework that repeatedly samples identical prompts under controlled conditions to surface latent failure modes and estimate per-inference failure probabilities.
Results: The application of APST to multiple instruction-tuned LLMs reveals substantial differences in empirical failure rates under repeated sampling, particularly as temperature increases, demonstrating the need for a practical framework to evaluate LLM safety and reliability under repeated inference.
Tags: Judge Reliability And Calibration, Benchmark And Dataset Creation
