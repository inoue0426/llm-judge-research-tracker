# LLM as a Judge â€” Daily Report

Report date: 2026-02-01 (UTC)
Coverage window: last 1 days

Query:
`ti:"LLM as a judge" OR abs:"LLM as a judge" OR ti:"LLM-as-a-judge" OR abs:"LLM-as-a-judge" OR ti:"large language model as a judge" OR abs:"large language model as a judge" OR ti:"LLM judge" OR abs:"LLM judge" OR ti:"LLM-based evaluation" OR abs:"LLM-based evaluation" OR ti:"LLM evaluation" OR abs:"LLM evaluation"`

Total papers: 2

## Papers

### PeerRank: Autonomous LLM Evaluation Through Web-Grounded, Bias-Controlled Peer Review (2602.02589v1)

Authors: Yanki Margalit, Erni Avram, Ran Taig, et al.
Published: 2026-02-01
Updated: 2026-02-01
Categories: cs.AI, cs.LG
Link: [arXiv](http://arxiv.org/abs/2602.02589v1)

Abstract: Evaluating large language models typically relies on human-authored benchmarks, reference answers, and human or single-model judgments, approaches that scale poorly, become quickly outdated, and mismatch open-world deployments that depend on web retrieval and synthesis. We introduce PeerRank, a fully autonomous end-to-end evaluation framework in which models generate evaluation tasks, answer them with category-scoped live web grounding, judge peer responses and aggregate dense peer assessments into relative performance estimates, without human supervision or gold references. PeerRank treats evaluation as a multi-agent process where each model participates symmetrically as task designer, respondent, and evaluator, while removing biased judgments. In a large-scale study over 12 commercially available models and 420 autonomously generated questions, PeerRank produces stable, discriminative rankings and reveals measurable identity and presentation biases. Rankings are robust, and mean peer scores agree with Elo. We further validate PeerRank on TruthfulQA and GSM8K, where peer scores correlate with objective accuracy. Together, these results suggest that bias-aware peer evaluation with selective web-grounded answering can scale open-world LLM assessment beyond static and human curated benchmarks.

Summary:
Purpose: The paper aims to introduce a fully autonomous end-to-end evaluation framework for large language models that can scale poorly and become outdated with traditional human-authored benchmarks and reference answers.
Method: The approach, called PeerRank, involves models generating evaluation tasks, answering them with live web grounding, judging peer responses, and aggregating dense peer assessments into relative performance estimates without human supervision or gold references.
Results: The study demonstrates that PeerRank produces stable and discriminative rankings, reveals measurable biases, and correlates with objective accuracy when validated on various datasets, suggesting its effectiveness in scaling open-world LLM assessment.
Tags: Benchmark And Dataset Creation, Judge Reliability And Calibration

### Small-Margin Preferences Still Matter-If You Train Them Right (2602.00954v1)

Authors: Jinlong Pang, Zhaowei Zhu, Na Di, et al.
Published: 2026-02-01
Updated: 2026-02-01
Categories: cs.AI
Link: [arXiv](http://arxiv.org/abs/2602.00954v1)

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

Summary:
Purpose: The paper revisits the assumption that small-margin preference pairs are noisy and should be filtered out, instead showing that they contain useful supervision signals when optimized correctly.
Method: The authors propose MixDPO, a difficulty-aware training strategy that orders preference data by difficulty and routes difficult pairs to a supervised fine-tuning objective while applying a preference loss to easy pairs.
Results: MixDPO consistently improves alignment over existing methods across three large language model benchmarks, with particularly strong gains on AlpacaEval~2 length-controlled win rate.
Tags: Benchmark And Dataset Creation
