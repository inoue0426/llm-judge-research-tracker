# LLM-as-a-Judge Report Subclasses

This note explains what the `llm-as-a-judge.md` report is doing and provides a practical, lightweight subclass scheme to help group papers while reading. The scheme is also used to generate structured tag outputs in CSV/JSON.

## What The Report Does

The report is generated by `scripts/collect_llm_judge_arxiv.py`. It queries arXiv for LLM-as-a-judge related terms, then summarizes each paper into three sentences (Purpose/Method/Results). If a local Ollama is reachable, it uses the LLM to produce those lines; otherwise it falls back to an abstract-based heuristic summary.

## Subclass Scheme (Example)

Use these subclasses as tags when scanning the report. Most papers will fit 1-2 tags. Each subclass has a stable key used in the CSV/JSON outputs.

1. Judge Prompting Protocols (`judge_prompting_protocols`)
   - Focus: How to prompt or structure the judge to improve reliability.
   - Signals: "rubrics", "criteria", "pairwise", "chain-of-thought", "calibration prompts".

2. Robustness And Sensitivity (`robustness_and_sensitivity`)
   - Focus: Sensitivity to prompt wording, paraphrases, or adversarial inputs.
   - Signals: "lexical/syntactic perturbations", "prompt sensitivity", "robustness".

3. Benchmark And Dataset Creation (`benchmarks_and_datasets`)
   - Focus: New datasets or benchmarks to evaluate LLMs using LLM judges.
   - Signals: "dataset", "benchmark", "curated", "collection", "taxonomy".

4. Judge Reliability And Calibration (`judge_reliability_and_calibration`)
   - Focus: Agreement with humans, bias analysis, or calibration of judge scores.
   - Signals: "inter-annotator", "alignment to human", "bias", "calibration".

5. Domain-Specific Judging (`domain_specific_judging`)
   - Focus: Applying LLM-as-a-judge to a specific domain.
   - Signals: "medical", "legal", "education", "software", "history", "policy".

6. Multi-Judge Or Ensemble Methods (`multi_judge_ensembles`)
   - Focus: Using multiple judges or models to reduce variance or bias.
   - Signals: "ensemble", "multi-judge", "majority vote", "consensus".

7. Metrics And Scoring Methods (`metrics_and_scoring`)
   - Focus: New scoring functions or evaluation metrics beyond raw judge ratings.
   - Signals: "metric", "score", "calibration curve", "probabilistic".

## How To Apply

1. Read the Purpose/Method/Results lines in `llm-as-a-judge.md`.
2. Assign 1-2 subclasses using the signals above.
3. If a paper is clearly mixed, choose the subclass that best matches the Method line.

## Notes

This scheme is intentionally simple. It is implemented in the script and exported to `llm-as-a-judge_tags.csv` and `llm-as-a-judge_tags.json`.
